{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 13: Entity & Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Relation extraction from Wikipedia articles\n",
    "\n",
    "Use Wikipedia to extract the relation `directedBy(Movie, Person)` by applying pattern based heuristics that utilize: *Part Of Speech Tagging*, *Named Entity Recognition* and *Regular Expressions*.\n",
    "\n",
    "#### Required Library: SpaCy\n",
    "- ```conda install -y spacy```\n",
    "- ```python -m spacy download en```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json, csv, re\n",
    "import spacy\n",
    "\n",
    "if int(spacy.__version__[0]) > 2:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "else:\n",
    "    nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tsv with input movies\n",
    "def read_tsv():\n",
    "    movies = []\n",
    "    with open(\"movies.tsv\", \"r\") as file:\n",
    "        tsv = csv.reader(file, delimiter=\"\\t\")\n",
    "        next(tsv)  # remove header\n",
    "        movies = [{\"movie\": line[0], \"director\": line[1]} for line in tsv]\n",
    "    return movies\n",
    "\n",
    "\n",
    "# parse wikipedia page\n",
    "def parse_wikipedia(movie):\n",
    "    txt = \"\"\n",
    "    try:\n",
    "        with urllib.request.urlopen(\n",
    "            \"https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro=&explaintext=&titles=\"\n",
    "            + movie\n",
    "        ) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            txt = next(iter(data[\"query\"][\"pages\"].values()))[\"extract\"]\n",
    "    except:\n",
    "        pass\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Parse the raw text of a Wikipedia movie page and extract named (PER) entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_PER_entities(txt):\n",
    "    txt = nlp(txt)\n",
    "\n",
    "    persons = []\n",
    "    for e in txt.ents:\n",
    "        if e.label_ == \"PERSON\":\n",
    "            persons.append(e.text)\n",
    "    return persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Given the raw text of a Wikipedia movie page and the extracted PER entities, find the director."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple heuristic: find the next PER entity after the word 'directed'\n",
    "def find_director(txt, persons):\n",
    "    txt = re.sub(\"[!?,.]\", \"\", txt).split()\n",
    "    for p1 in range(0, len(txt)):\n",
    "        if txt[p1] == \"directed\":\n",
    "            for p2 in range(p1, len(txt)):\n",
    "                for per in persons:\n",
    "                    if per.startswith(txt[p2]):\n",
    "                        return per\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = read_tsv()\n",
    "\n",
    "fp = 0\n",
    "statements = []\n",
    "for m in movies:\n",
    "    txt = parse_wikipedia(m[\"movie\"])\n",
    "    persons = find_PER_entities(txt)\n",
    "    director = find_director(txt, persons)\n",
    "\n",
    "    if director != \"\":\n",
    "        statements.append(m[\"movie\"] + \" is directed by \" + director + \".\")\n",
    "        if director != m[\"director\"]:\n",
    "            fp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Compute the precision and recall based on the given ground truth (column Director from tsv file) and show examples of statements that are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 77%\n",
      "Recall: 78%\n",
      "\n",
      "***Sample Statements***\n",
      "13_Assassins_(2010_film) is directed by Takashi Miike.\n",
      "14_Blades is directed by Daniel Lee.\n",
      "22_Bullets is directed by Richard Berry.\n",
      "Alien_vs_Ninja is directed by Seiji Chiba.\n",
      "Bad_Blood_(2010_film) is directed by Dennis Law.\n"
     ]
    }
   ],
   "source": [
    "# compute precision and recall\n",
    "fn = len(movies) - len(statements)\n",
    "tp = len(statements) - fp\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(\"Precision: {:.0%}\".format(precision))\n",
    "print(\"Recall: {:.0%}\".format(recall))\n",
    "\n",
    "print()\n",
    "print(\"***Sample Statements***\")\n",
    "for s in statements[:5]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Named Entity Recognition using Hidden Markov Model\n",
    "\n",
    "\n",
    "Define a Hidden Markov Model (HMM) that recognizes Person (*PER*) entities.\n",
    "Particularly, your model must be able to recognize pairs of the form (*firstname lastname*) as *PER* entities.\n",
    "Using the given sentences as training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = [\n",
    "    \"The best blues singer was Bobby Bland while Ray Charles pioneered soul music .\",\n",
    "    \"Bobby Bland was just a singer whereas Ray Charles was a pianist , songwriter and singer .\",\n",
    "    \"None of them lived in Chicago .\",\n",
    "]\n",
    "\n",
    "test_set = [\n",
    "    \"Ray Charles was born in 1930 .\",\n",
    "    \"Bobby Bland was born the same year as Ray Charles .\",\n",
    "    \"Muddy Waters is the father of Chicago Blues .\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Annotate your training set with the labels I (for PER entities) and O (for non PER entities).\n",
    "\t\n",
    "    *Hint*: Represent the sentences as sequences of bigrams, and label each bigram.\n",
    "\tOnly bigrams that contain pairs of the form (*firstname lastname*) are considered as *PER* entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation\n",
      "[['The best', 'O'], ['best blues', 'O'], ['blues singer', 'O'], ['singer was', 'O'], ['was Bobby', 'O'], ['Bobby Bland', 'I'], ['Bland while', 'O'], ['while Ray', 'O'], ['Ray Charles', 'I'], ['Charles pioneered', 'O'], ['pioneered soul', 'O'], ['soul music', 'O'], ['music .', 'O']]\n",
      "[['Bobby Bland', 'I'], ['Bland was', 'O'], ['was just', 'O'], ['just a', 'O'], ['a singer', 'O'], ['singer whereas', 'O'], ['whereas Ray', 'O'], ['Ray Charles', 'I'], ['Charles was', 'O'], ['was a', 'O'], ['a pianist', 'O'], ['pianist ,', 'O'], [', songwriter', 'O'], ['songwriter and', 'O'], ['and singer', 'O'], ['singer .', 'O']]\n",
      "[['None of', 'O'], ['of them', 'O'], ['them lived', 'O'], ['lived in', 'O'], ['in Chicago', 'O'], ['Chicago .', 'O']]\n"
     ]
    }
   ],
   "source": [
    "# Bigram Representation\n",
    "def getBigrams(sents):\n",
    "    return [\n",
    "        [b[0] + \" \" + b[1] for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "        for l in sents\n",
    "    ]\n",
    "\n",
    "\n",
    "bigrams = getBigrams(training_set)\n",
    "\n",
    "# Annotation\n",
    "PER = [\"Bobby Bland\", \"Ray Charles\"]\n",
    "annotations = [\n",
    "    [[b, \"I\" if b in PER else \"O\"] for b in sentence] for sentence in bigrams\n",
    "]\n",
    "\n",
    "print(\"Annotation\")\n",
    "for annotated_sentence in annotations:\n",
    "    print(annotated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Compute the transition and emission probabilities for the HMM (use smoothing parameter $\\lambda$=0.5).\n",
    "\n",
    "    *Hint*: For the emission probabilities you can utilize the morphology of the words that constitute a bigram (e.g., you can count their uppercase first characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probabilities\n",
      "{'P(I|start)': 0.3333333333333333, 'P(O|start)': 0.6666666666666667, 'P(O|O)': 0.8928571428571429, 'P(O|I)': 1.0, 'P(I|O)': 0.10714285714285714, 'P(I|I)': 0.0}\n",
      "\n",
      "Emission Probabilities\n",
      "{'P(2_upper|O)': 0.014285714285714285, 'P(2_upper|I)': 0.5142857142857142, 'P(1_upper|O)': 0.21071428571428572, 'P(1_upper|I)': 0.014285714285714285, 'P(0_upper|O)': 0.37142857142857144, 'P(0_upper|I)': 0.014285714285714285}\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.5\n",
    "\n",
    "# Transition Probabilities\n",
    "transition_prob = {}\n",
    "\n",
    "I_count = sum(\n",
    "    a[1] == \"I\" for annotated_sentence in annotations for a in annotated_sentence[:-1]\n",
    ")\n",
    "O_count = sum(\n",
    "    a[1] == \"O\" for annotated_sentence in annotations for a in annotated_sentence[:-1]\n",
    ")\n",
    "\n",
    "# Initial probabilities\n",
    "transition_prob[\"P(I|start)\"] = float(\n",
    "    sum(annotated_sentence[0][1] == \"I\" for annotated_sentence in annotations)\n",
    ") / len(annotations)\n",
    "transition_prob[\"P(O|start)\"] = 1 - transition_prob[\"P(I|start)\"]\n",
    "\n",
    "\n",
    "O_after_O_count = 0\n",
    "O_after_I_count = 0\n",
    "I_after_O_count = 0\n",
    "I_after_I_count = 0\n",
    "for annotated_sentence in annotations:\n",
    "    for i, (_, token) in enumerate(annotated_sentence[:-1]):\n",
    "        next_token = annotated_sentence[i + 1][1]\n",
    "        if token == \"O\":\n",
    "            if next_token == \"O\":\n",
    "                O_after_O_count += 1\n",
    "            else:\n",
    "                I_after_O_count += 1\n",
    "        else:\n",
    "            if next_token == \"O\":\n",
    "                O_after_I_count += 1\n",
    "            else:\n",
    "                I_after_I_count += 1\n",
    "\n",
    "transition_prob[\"P(O|O)\"] = O_after_O_count / O_count\n",
    "transition_prob[\"P(O|I)\"] = O_after_I_count / I_count\n",
    "transition_prob[\"P(I|O)\"] = I_after_O_count / O_count\n",
    "transition_prob[\"P(I|I)\"] = I_after_I_count / I_count\n",
    "\n",
    "print(\"Transition Probabilities\", transition_prob, sep=\"\\n\")\n",
    "print()\n",
    "\n",
    "# Emission Probabilities\n",
    "emission_prob = {}\n",
    "\n",
    "\n",
    "def count_upper_first_char(bigram):\n",
    "    count = 0\n",
    "    if bigram.split(\" \")[0][0].isupper():\n",
    "        count += 1\n",
    "    if bigram.split(\" \")[1][0].isupper():\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "both_upper_count_O = 0\n",
    "both_upper_count_I = 0\n",
    "one_upper_count_O = 0\n",
    "one_upper_count_I = 0\n",
    "no_upper_count_O = 0\n",
    "no_upper_count_I = 0\n",
    "for a in sum(annotations, []):\n",
    "    if count_upper_first_char(a[0]) == 2 and a[1] == \"O\":\n",
    "        both_upper_count_O += 1\n",
    "    elif count_upper_first_char(a[0]) == 2 and a[1] == \"I\":\n",
    "        both_upper_count_I += 1\n",
    "    elif count_upper_first_char(a[0]) == 1 and a[1] == \"O\":\n",
    "        one_upper_count_O += 1\n",
    "    elif count_upper_first_char(a[0]) == 1 and a[1] == \"I\":\n",
    "        one_upper_count_I += 1\n",
    "    elif count_upper_first_char(a[0]) == 0 and a[1] == \"O\":\n",
    "        no_upper_count_O += 1\n",
    "    elif count_upper_first_char(a[0]) == 0 and a[1] == \"I\":\n",
    "        no_upper_count_I += 1\n",
    "\n",
    "\n",
    "default_emission = 1 / len(sum(bigrams, [])) * (1 - lambda_)\n",
    "\n",
    "emission_prob[\"P(2_upper|O)\"] = (\n",
    "    both_upper_count_O / O_count\n",
    ") * lambda_ + default_emission\n",
    "emission_prob[\"P(2_upper|I)\"] = (\n",
    "    both_upper_count_I / I_count\n",
    ") * lambda_ + default_emission\n",
    "emission_prob[\"P(1_upper|O)\"] = (\n",
    "    one_upper_count_O / O_count\n",
    ") * lambda_ + default_emission\n",
    "emission_prob[\"P(1_upper|I)\"] = (\n",
    "    one_upper_count_I / I_count\n",
    ") * lambda_ + default_emission\n",
    "emission_prob[\"P(0_upper|O)\"] = (\n",
    "    no_upper_count_O / O_count\n",
    ") * lambda_ + default_emission\n",
    "emission_prob[\"P(0_upper|I)\"] = (\n",
    "    no_upper_count_I / I_count\n",
    ") * lambda_ + default_emission\n",
    "\n",
    "print(\"Emission Probabilities\", emission_prob, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Predict the labels of the test set and compute the precision and the recall of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Entities\n",
      " ['Ray Charles', 'Bobby Bland', 'Ray Charles', 'Muddy Waters', 'Chicago Blues'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "bigrams = getBigrams(test_set)\n",
    "entities = []\n",
    "for sentence in bigrams:\n",
    "    prev_state = \"start\"\n",
    "    for b in sentence:\n",
    "        I_prob = (\n",
    "            transition_prob[\"P(I|\" + prev_state + \")\"]\n",
    "            * emission_prob[\"P(\" + str(count_upper_first_char(b)) + \"_upper|I)\"]\n",
    "        )\n",
    "        O_prob = (\n",
    "            transition_prob[\"P(O|\" + prev_state + \")\"]\n",
    "            * emission_prob[\"P(\" + str(count_upper_first_char(b)) + \"_upper|O)\"]\n",
    "        )\n",
    "\n",
    "        if I_prob > O_prob:\n",
    "            entities.append(b)\n",
    "            prev_state = \"I\"\n",
    "        else:\n",
    "            prev_state = \"O\"\n",
    "\n",
    "print(\"Predicted Entities\\n\", entities, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is *80%* while recall is *100%*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Comment on how you can further improve this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could increase precision by computing also the probabilities for unigrams and averaging them in the prediction step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "228px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
