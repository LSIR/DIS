{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Information Retrieval\n",
    "\n",
    "In this exercise we implement a core process of Fagin's algorithm, the parallel scanning of the posting lists. Assume an aggregation function that returns the sum of the tf-idf scores given the terms in a document.\n",
    "\n",
    "We assume that the posting lists for each term of a retrieval system are running on a different node.\n",
    "\n",
    "We first create a dictionary $h$, where each entry of the dictionary corresponds to a posting list for term, assumed to be hosted in a different node. Explore the structure of $h$, to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/duong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "# with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "    \n",
    "def search_vec(query, k):\n",
    "    query_vector = vectorize_query(query, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    ans = []\n",
    "    indices = []\n",
    "    for i in range(min(k,len(original_documents))):\n",
    "        ans.append(original_documents[scores[i][1]])\n",
    "        indices.append(scores[i][1])\n",
    "    return ans, indices, query_vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "In this exercise we implement a core process of Fagin's algorithm, the parallel scanning of the posting lists. Assume an aggregation function that returns the sum of the tf-idf scores given the terms in a document.\n",
    "\n",
    "We assume that the posting lists for each term of a retrieval system are running on a different node.\n",
    "\n",
    "We first create a dictionary $h$, where each entry of the dictionary corresponds to a posting list for a term, assumed to be hosted in a different node. Explore the structure of $h$, to understand it. We suggest to first use the simple collection breads.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "doc_vecs = np.transpose(np.array(document_vectors))\n",
    "h = {}\n",
    "for i, term in enumerate(vocabulary):\n",
    "    ha = {}\n",
    "    for docj in range(len(original_documents)):\n",
    "        tfidf = doc_vecs[i][docj]\n",
    "        ha[docj] = tfidf\n",
    "    sorted_ha = sorted(ha.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    h[term] = sorted_ha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the following function that implements the Fagin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 2, 2: 1, 0: 2, 1: 1}\n",
      "[(3, 1.1394342831883648), (0, 0.5697171415941824)]\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
      "How to Bake Breads Without Baking Recipes\n"
     ]
    }
   ],
   "source": [
    "def fagin_algorithm(query, h, k, vocabulary):\n",
    "    \n",
    "    # Split and stem the query\n",
    "    q = query.split()\n",
    "    q = set([stemmer.stem(w) for w in q])\n",
    "    query_term_cnt = len(q)\n",
    "    \n",
    "    # select the posting lists for the query terms\n",
    "    posting_lists = {}\n",
    "    for term in q:\n",
    "        if term in h:\n",
    "            posting_lists[term] = h[term]\n",
    "    \n",
    "    max_len = len(documents)\n",
    "    \n",
    "    # Traverse the selected posting lists until we found k documents that appear in ALL posting lists\n",
    "    # This corresponds to phase 1 of Fagin's algorithm.\n",
    "    # As a result you produce a dictionary documents_occurrences, with the document identifiers as keys, \n",
    "    # and the number of documents found as value.\n",
    "    # We stop traversing the posting lists until we have found k documents that appear in ALL posting lists \n",
    "    # of the query terms\n",
    "\n",
    "    documents_occurrences = {}\n",
    "    l = 0\n",
    "    found_documents = 0\n",
    "    while l < max_len:\n",
    "        for term in q:\n",
    "            d  = posting_lists[term][l][0]\n",
    "            if d in documents_occurrences.keys():\n",
    "                documents_occurrences[d] = documents_occurrences[d]+1\n",
    "            else:\n",
    "                documents_occurrences[d] = 1\n",
    "            if documents_occurrences[d] == query_term_cnt:\n",
    "                found_documents = found_documents + 1\n",
    "        if found_documents == k:\n",
    "            l = max_len + 1\n",
    "            break\n",
    "        else:\n",
    "            l = l+1\n",
    "                \n",
    "    print(documents_occurrences)\n",
    "        \n",
    "    # Retrieve for the found documents the tfidf values and add them up.\n",
    "    # For simplicity, we do not distinguish the cases whether the values have already been retrieved in the \n",
    "    # previous phase, or whether we use random access to obtain those values\n",
    "    \n",
    "    tfidf = {}\n",
    "    for d in documents_occurrences.keys():\n",
    "        t = 0\n",
    "        for term in q:\n",
    "            t = t + dict(posting_lists[term])[d]\n",
    "        tfidf[d] = t\n",
    "        \n",
    "    # Finally we compute the top-k documents and return the answer\n",
    "    \n",
    "    ans = sorted(tfidf.items(), key=lambda x:x[1], reverse = True)[:k]\n",
    "    return ans\n",
    "\n",
    "\n",
    "ans = fagin_algorithm(\"bread recipe\", h, 2, vocabulary)\n",
    "print(ans)\n",
    "for document_id in ans:\n",
    "    print(original_documents[document_id[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce your own datasets to explore the behavior of the algorithm. Create a dataset such that for retrieving a 2 term query a total of 6 documents needs to be retrieved in the first phase of the algorithm (as in the example in the lecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Retrieval\n",
    "### Question a\n",
    "\\begin{align}\n",
    "P(q|d_j) & = \\frac{P(q \\cap d_j)}{P(d_j)} \\\\\n",
    "& = \\sum_{i=1}^m \\frac{P(q \\cap d_j | k_i) P(k_i)}{P(d_j)} & \\text{using $P(A) = \\sum_B P(A|B) P(B)$} \\\\\n",
    "& = \\sum_{i=1}^m \\frac{P(q|k_i) P(d_j | k_i) P(k_i)}{P(d_j)} & \\text{using conditional independence} \\\\\n",
    "& = \\sum_{i=1}^m P(k_i |d_j) P(q|k_i) & \\text{using Bayes theorem, $P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question b\n",
    "$P(q|d_1) = \\sum_{i=1}^m P(k_i|d_1) P(q|k_i) = 0 \\times \\frac{1}{5} + \\frac{1}{3} \\times 0 + \\frac{2}{3} \\frac{2}{3} = \\frac{4}{9} = 0.4444$\n",
    "\n",
    "Similarly, for the remaining documents:\n",
    "+ $P(q|d_2) = 1/15 = 0.0666$\n",
    "+ $P(q|d_3) = 11/30 = 0.3666$\n",
    "+ $P(q|d_4) = 3/20 = 0.15$. \n",
    "Thus the final ranking is $d_1, d_3, d_4, d_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question c\n",
    "* Similarity\n",
    " - For retrieval, both approaches aim to measure the relevance between the query q and document $d_i$ by computing $P(q|d_i)$\n",
    "* Diference\n",
    " - They are different in how $P(q|d)$ is computed. In the above, $P(q|d) = \\sum_{i=1}^m P(k_i|d) P(q|k_i)$ while in the lecture $P(q|d) = \\prod_{i=1}^m P(k_i|M_{d}) = \\prod_{i=1}^m \\frac{tf(k_i,d)}{L_d}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
