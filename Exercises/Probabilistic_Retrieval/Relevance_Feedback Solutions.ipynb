{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 2: Relevance Feedback - SOLUTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/elmas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "#with open(\"bread.txt\") as f:\n",
    "with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "#original_documents = [x.decode('utf-8').strip() for x in content] # for python2\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "    \n",
    "def search_vec(query, k):\n",
    "    query_vector = vectorize_query(query, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    ans = []\n",
    "    indices = []\n",
    "    for i in range(min(k,len(original_documents))):\n",
    "        ans.append(original_documents[scores[i][1]])\n",
    "        indices.append(scores[i][1])\n",
    "    return ans, indices, query_vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Implement Rocchio's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, alpha, beta, gamma):\n",
    "    # Note: relevant_doc_vecs and non_relevant_doc_vecs are list of vectors, vectors are also lists in this case. \n",
    "    # We are using (zip(*list)) to columnwise addition. i.e. [[1,2,3], [4,5,6]] iterate over tuples (1,4),(2,5),(3,6)\n",
    "    # Check here: https://stackoverflow.com/questions/29139350/difference-between-ziplist-and-ziplist\n",
    "    # You can use numpy if you want to go fancier\n",
    "    \n",
    "    num_rel = len(relevant_doc_vecs)\n",
    "    num_non_rel = len(non_relevant_doc_vecs)\n",
    "    \n",
    "    # Compute the first term in the Rocchio equation\n",
    "    norm_query_vector = [alpha * weight for weight in query_vector]\n",
    "    \n",
    "    # Compute the second term in the Rocchio equation\n",
    "    norm_sum_relevant = [beta*sum(x)/num_rel for x in zip(*relevant_doc_vecs)]\n",
    "    \n",
    "    # Compute the last term in the Rocchio equation\n",
    "    norm_sum_non_relevant = [-gamma*sum(x)/num_non_rel for x in zip(*non_relevant_doc_vecs)]\n",
    "    \n",
    "    # Sum all the terms\n",
    "    modified_query_vector = [sum(x) for x in zip(norm_sum_relevant, norm_sum_non_relevant, norm_query_vector)]\n",
    "    \n",
    "    # Ignore negative elements\n",
    "    modified_query_vector = [x if x>0 else 0 for x in modified_query_vector]\n",
    "    return modified_query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Test Rocchio's method on the `computer science` query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ\n",
      "1 New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD\n",
      "2 An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj\n",
      "3 New at @epfl_en Life Sciences @epflSV: \"From PhD directly to Independent Group Leader\" #ELFIR_EPFL:  Early Independence Research Scholars. See https://t.co/evqyqD7FFl, also for computational biology #compbio https://t.co/e3pDCg6NVb Deadline April 1 2018 at https://t.co/mJqcrfIqkb\n",
      "4 Video of Nicola Marzari from @EPFL_en  on Computational Discovery in the 21st Century during #PASC17 now online: https://t.co/tfCkEvYKtq https://t.co/httPdHcK9W\n"
     ]
    }
   ],
   "source": [
    "ans, result_doc_ids, query_vector = search_vec(\"computer science\", 5)\n",
    "for i in range(len(ans)):\n",
    "    print(i,ans[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluate the method of variation of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified query:  20162017 amp barth biophys combin comput congrat control distanc epfl eth excit httpstcoarslxzoshq httpstcob9tglxo4kd httpstcoijwbaebocj httpstcoznjk3bz6mo interview model new news no1 no8 patrick professor protein rank scienc show subject univers vdtech via world\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ',\n",
       " 'New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD',\n",
       " 'An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj',\n",
       " 'International ranking: #ETH and #EPFL are top institutes',\n",
       " 'Interview (in French) de Patrick Aebischer, un \"innovation slasher\" @EPFL_en https://t.co/BtzhxEAEmN',\n",
       " 'The proteins that domesticated our genomes https://t.co/npGbUKJhBl  via @EPFL_en #VDtech https://t.co/It0SBqlKQc',\n",
       " \"New software can model natural light from the occupants' perspective https://t.co/RbMmN3Po5v via @EPFL_en #VDtech https://t.co/50enZtwUHi\",\n",
       " \"New software can model natural light from the occupants' perspective https://t.co/RbMmN3Po5v via @EPFL_en #VDtech https://t.co/lLIAvntc9R\",\n",
       " 'Latest work in our lab shows how feedback enhances brainwave control of a novel hand-exoskeleton https://t.co/VVPdX19fIM #epfl',\n",
       " 'â€œArtificial intelligence has the potential to revolutionize transportation\" interview of Alexandre Alahi, new professor of Transportation Engineering @EPFL https://t.co/S8ZKRIAHbq RT @Trace_EPFL https://t.co/PQqkRY5ny9']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of indices marked as relevant\n",
    "#Â suppose first three documents were relevant and the rest were irrelevant.\n",
    "relevant_indices = [0,1,2]\n",
    "non_relevant_indices = [i for i in range(3, len(ans))]\n",
    "\n",
    "relevant_doc_ids = [result_doc_ids[i] for i in relevant_indices]\n",
    "non_relevant_doc_ids = [result_doc_ids[i] for i in non_relevant_indices]\n",
    "\n",
    "relevant_doc_vecs = [document_vectors[i] for i in relevant_doc_ids]\n",
    "non_relevant_doc_vecs = [document_vectors[i] for i in non_relevant_doc_ids]\n",
    "\n",
    "expanded_query = expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, 1, 1, 1)\n",
    "\n",
    "new_query = ' '.join([vocabulary[i] for i, val in enumerate(expanded_query) if val>0])\n",
    "\n",
    "new_ans , not_important_now, idontcare_anymore = search_vec(new_query, 10)\n",
    "\n",
    "print('Modified query: ', new_query)\n",
    "new_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
