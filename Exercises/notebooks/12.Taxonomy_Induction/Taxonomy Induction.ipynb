{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Taxonomy Induction\n",
    "\n",
    "<br>\n",
    "In this exercise, we will perform the various steps commonly employed for unsupervised taxonomy induction from text corpora. Taxonomy induction from text typically consists of three main steps:\n",
    "\n",
    "<ol>\n",
    "  <li><b>Relations Extraction:</b> In this step, we use lexico-syntactic patterns to extract <b>IsA</b> relations from text. An example of IsA relation is (<i>apple, fruit</i>), which implies that <i>apple</i> is a type of fruit.</li>\n",
    "  <br>\n",
    "  <li><b>Initial Graph Construction:</b> In this step, we aggregate the extracted IsA relations to construct a potentially-noisy initial hypernym graph.</li>\n",
    "  <br>\n",
    "  <li><b>Graph Pruning:</b> In the final step, we perform some pruning or optimization steps to induce the a final clean taxonomy.</li>\n",
    "</ol>  \n",
    "\n",
    "<br>\n",
    "We will describe these steps in detail in the rest of this exercise. \n",
    "\n",
    "\n",
    "## Question 1 - Relations Extraction\n",
    "\n",
    "\n",
    "In this part of the exercise, we will run a small-scale extraction of IsA relations and inspect the results. Relations extraction uses lexico-syntactic patterns to identify IsA relations from unstructured text. Examples of lexico-syntactic patterns include:\n",
    "\n",
    " Lexico-syntactic pattern | Sample matching text\n",
    "  ------|------------------\n",
    "  <b>X</b> is a <b>Y</b>     | <i><b>apple</b> is a <b>fruit</b></i>, <i><b>switzerland</b> is a <b>country</b></i>\n",
    "  <b>X</b> such as <b>Y</b>     | <i><b>fruits</b> such as <b>mango</b></i>, <i><b>scientists</b> such as <b>Einstein</b></i>\n",
    "  <b>X</b> is an example of <b>Y</b>     | <i><b>iphone</b> is an example of <b>smartphone</b></i>\n",
    "  \n",
    "  <br>\n",
    "  In this exercise, we will use such lexico-syntactic patterns to identify IsA relations from text.\n",
    "  \n",
    "  \n",
    "  ###  Question 1.a\n",
    "\n",
    "  Load the given file <b>wiki_food_en.txt</b> into memory using the following code:\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(filename):\n",
    "    file_text = []\n",
    "    with open(filename) as fp:\n",
    "        for line in fp:\n",
    "            file_text.append(line.strip().lower())\n",
    "    return \" \".join(file_text)\n",
    "\n",
    "file_text = load_text_file(\"wiki_food_en.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The following code uses the regular expression library to detect lexico-syntactic patterns in the file_text. The example below uses the regular expression \"X is a Y\". Fill in the blanks (...):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_matches(file_text, regexp_string):\n",
    "    #Compile a regular expression \n",
    "    regexp = re.compile(...)\n",
    "    \n",
    "    #Find all matches with the given regular expression\n",
    "    matches = ...(regexp, file_text)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "isa_matches = find_matches(file_text, \"[a-z]+ is a ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    " ###  Question 1.b\n",
    "\n",
    "Run the above code for relations extraction with the following lexico-syntactic patterns:\n",
    "\n",
    "<ol>\n",
    "  <li>X such as Y</li>\n",
    "  <li>such X as Y</li>\n",
    "<li>X and other Y</li>  \n",
    "</ol>\n",
    "\n",
    "Manually inspect the results and compute the accuracies of first 20 matches for each lexico-syntactic pattern. What do you observe? Is there any important difference between patterns no. 1,2 and 3? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Graph Construction\n",
    "\n",
    "As you noticed in the previous step, the output of lexico-syntactic patterns-based relations extraction contains significant noise. The task of noise removal is fairly involved and beyond the scope of this exercise. For further reading, we recommend this paper, which demonstrates a state-of-the-art effort for IsA relations extraction (<a href=\"http://webdatacommons.org/isadb/lrec2016.pdf\">A Large Database of Hypernymy Relations Extracted from the Web</a>).\n",
    "\n",
    "In this part of the exercise, we assume that IsA relations extracted using a state-of-the-art approach are already available. Given these relations, the aim of this step is to construct an initial potentially-noisy hypernym graph.\n",
    "\n",
    "###  Question 2.a\n",
    "\n",
    "\n",
    "Load the IsA relations of the food domain from the given file \"food_isa_relations.txt\" using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = []\n",
    "with open(\"food_isa_relations.txt\") as fp:\n",
    "    for line in fp.readlines():\n",
    "        toks = line.strip().split('\\t')\n",
    "        rels.append((toks[0],toks[1],float(toks[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, graphs are better handled as a default 2-level dictionary. For example, the edge (<i>apple</i>,<i>fruit</i>, freq) is represented as a two-level map:\n",
    "\n",
    "map['apple']['fruit'] = freq\n",
    "\n",
    "The following code converts the IsA relations loaded from the file into a 2-level dictionary. Fill in the blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "noisy_relations = defaultdict(dict)\n",
    "for hypo, hyper, freq in rels:\n",
    "    noisy_relations[...][...] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "\n",
    "###  Question 2.b\n",
    "\n",
    "The next step of taxonomy induction involves removing and filtering out noisy IsA relations. In a real scenario, this usually involves a wide variety of steps. However, in this exercise, we will implement only one step. In this step, we will sort all the hypernyms for each hyponym, and only retain top-5 hypernyms for each hyponym.\n",
    "\n",
    "First, print the hypernyms of 'apple':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_relations['apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Fill in the blanks in the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hypo in noisy_relations.key():\n",
    "    sorted_hypernyms = sorted(..., key = lambda x: ...)\n",
    "    noisy_relations[hypo] = {k:v for k,v in ...}\n",
    "    \n",
    "# Printed filtered noisy relations.\n",
    "noisy_relations['apple']   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 2.c\n",
    "\n",
    "In the next step, we would first convert the set of filtered IsA relations into a graph. First install the library networkx and matplotlib:\n",
    "\n",
    "   $ pip install networkx<br>\n",
    "   \n",
    "   $ pip install matplotlib\n",
    "   \n",
    "\n",
    "\n",
    "Further, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx and nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G=nx.DiGraph()\n",
    "\n",
    "for hypo in noisy_relations.keys():\n",
    "    for hyper in noisy_relations[hypo].keys():\n",
    "        G.add_edge(hypo, hyper)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Print all the paths between the following terms (Hint: use the networkx function all_simple_paths):\n",
    "\n",
    "1. 'apple' and 'food'\n",
    "2. 'fusilli' and 'food'\n",
    "3.  'okra' and 'food' \n",
    "\n",
    "Do you notice any relationship between the length of the path and its accuracy?\n",
    "\n",
    "\n",
    "###  Question 2.d\n",
    "\n",
    "In this step, we will now build a taxonomy. We will undertake the following steps:\n",
    "\n",
    "<ol>\n",
    "  <li> Let the vocabulary be {'apple', 'mango', 'peach', 'orange', 'banana'}.</li> \n",
    "<li> Let the root of the taxonomy be 'food'.</li> \n",
    "<li> Find all simple paths between terms in the vocabulary and the root. </li>\n",
    "<li> Retain all simple paths of length $l$.\n",
    "<li> Construct a graph by aggregatiing all the edges in the retained paths. </li>\n",
    "\n",
    "The below code implements the above steps. Fill in the blanks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_paths(vocab, root, l):\n",
    "    retained_paths = []\n",
    "    for term in vocab:\n",
    "        for path in ...:\n",
    "            if ...:\n",
    "                retained_paths.append(path)\n",
    "    return retained_paths\n",
    "\n",
    "\n",
    "def aggregate_paths(paths):\n",
    "    agg_graph = defaultdict(dict)\n",
    "    \n",
    "    for path in paths:\n",
    "        for i,term in enumerate(path[0:len(path) -1]):\n",
    "            agg_graph[...][...] = 1\n",
    "            \n",
    "    return agg_graph\n",
    "\n",
    "\n",
    "V = ['apple', 'mango', 'peach', 'orange', 'banana']\n",
    "root = 'food'\n",
    "\n",
    "graph = aggregate_paths(select_paths(V, root, 3))\n",
    " \n",
    "\n",
    "# Plot the graph\n",
    "Gt = nx.DiGraph()\n",
    "for k in graph.keys():\n",
    "    for k1 in graph[k].keys():\n",
    "        Gt.add_edge(k,k1)\n",
    "            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 2.e\n",
    "\n",
    "Plot the aggregated graph using the previous steps but with different path lengths (For example, 2 or 4). What do you notice?\n",
    "\n",
    "###  Question 2.f\n",
    "\n",
    "Repeat steps from 2.b to 2.e but without filtering the noisy relations in step 2.b. What do you observe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
