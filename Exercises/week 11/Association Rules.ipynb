{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will implement two techniques that are part of the so-called shopping basket analysis, which will help us to better understand how customers data are being processed to extract insights about their habits.\n",
    "\n",
    "\n",
    "#### Notes about external libraries\n",
    "You can check your implementation of the Apriori algorithm and the Association Rules using MLxtend, a data mining library. Unfortunately, the library is not directly shipped with Anaconda. To install MLxtend, just execute  \n",
    "\n",
    "```bash\n",
    "pip install mlxtend  \n",
    "```\n",
    "\n",
    "Or directly using Anaconda\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge mlxtend \n",
    "```\n",
    "\n",
    "Note that the installation of MLxtend is not mandatory, as we will provide the expected results in pre-rendered cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 1: Apriori algorithm\n",
    "In the first excercise, we will put into practice the Apriori algorithm. In particular, we will extract frequent itemsets from a list of transactions coming from a grocery store. You will have to complete the function `get_support(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Format the transaction dataset.\n",
    "Expect a list of transaction in the format:\n",
    "[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], ...]\n",
    "\"\"\"\n",
    "def preprocess(dataset):\n",
    "    unique_items = set()\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            unique_items.add(item)\n",
    "       \n",
    "    # Converting to frozensets to use itemsets as dict key\n",
    "    unique_items = [frozenset([i]) for i in list(unique_items)]\n",
    "    \n",
    "    return unique_items,list(map(set,dataset))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generate candidates of length n+1 from a list of items, each of length n.\n",
    "\n",
    "Example:\n",
    "[{1}, {2}, {5}]          -> [{1, 2}, {1, 5}, {2, 5}]\n",
    "[{2, 3}, {2, 5}, {3, 5}] -> [{2, 3, 5}]\n",
    "\"\"\"\n",
    "def generate_candidates(Lk):\n",
    "    output = []\n",
    "\n",
    "    # We generate rules of the target size k\n",
    "    k=len(Lk[0])+1\n",
    "    \n",
    "    for i in range(len(Lk)):\n",
    "        for j in range(i+1, len(Lk)): \n",
    "            L1 = list(Lk[i])[:k-2]; \n",
    "            L2 = list(Lk[j])[:k-2]\n",
    "            L1.sort(); \n",
    "            L2.sort()\n",
    "\n",
    "            # Merge sets if first k-2 elements are equal\n",
    "            # For the case of k<2, generate all possible combinations\n",
    "            if L1==L2: \n",
    "                output.append(Lk[i] | Lk[j])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Print the results of the apriori algorithm\n",
    "\"\"\"\n",
    "def print_support(support,max_display=10,min_items=1):\n",
    "    print('support\\t itemset')\n",
    "    print('-'*30)\n",
    "    filt_support = {k:v for k,v in support.items() if len(k)>=min_items}\n",
    "    for s,sup in sorted(filt_support.items(), key=operator.itemgetter(1),reverse=True)[:max_display]:\n",
    "        print(\"%.2f\" % sup,'\\t',set(s))\n",
    "        \n",
    "def print_support_mx(df,max_display=10,min_items=1):\n",
    "    print('support\\t itemset')\n",
    "    print('-'*30)\n",
    "    lenrow = df['itemsets'].apply(lambda x: len(x))\n",
    "    df  = df[lenrow>=min_items]\n",
    "    df  = df.sort_values('support',ascending=False).iloc[:max_display]\n",
    "    for i,row in df.iterrows():\n",
    "        print(\"%.2f\" % float(row['support']),'\\t',set(row['itemsets']))\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Run the apriori algorithm\n",
    "\n",
    "dataset     : list of transactions\n",
    "min_support : minimum support. Itemsets with support below this threshold\n",
    "              will be pruned.\n",
    "\"\"\"\n",
    "def apriori(dataset, min_support = 0.5):\n",
    "    unique_items,dataset = preprocess(dataset)\n",
    "    L1, supportData      = get_support(dataset, unique_items, min_support)\n",
    "    \n",
    "    L = [L1]\n",
    "    k = 0\n",
    "    while True:\n",
    "        Ck       = generate_candidates(L[k])\n",
    "        Lk, supK = get_support(dataset, Ck, min_support)\n",
    "        \n",
    "        # Is there itemsets of length k that have the minimum support ?\n",
    "        if len(Lk)>0:\n",
    "            supportData.update(supK)\n",
    "            L.append(Lk) \n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return L, supportData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "The [Apriori Algorithm](https://en.wikipedia.org/wiki/Apriori_algorithm) identifies frequent combinations of items by extending them to larger and larger itemsets (see the generate_candidates function) as long as they appear sufficiently often in a list of transactions.\n",
    "\n",
    "Compute support for all the candidate itemsets contained in Ck, given the total list of transactions. We already provide the functions to compute candidate itemsets. The support of the itemset $X$ with respect to the list of transactions $T$ is defined as the proportion of transactions $t$ in the dataset which contains the itemset $X$. Support can be computed using the following formula\n",
    "\n",
    "$$\\mathrm{supp}(X) = \\frac{|\\{t \\in T; X \\subseteq t\\}|}{|T|}$$  \n",
    "\n",
    "After computing the support for each itemset, prune the ones that do not match the minimal specificied support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute support for each provided itemset by counting the number of\n",
    "its occurences in the original dataset of transactions.\n",
    "\n",
    "dataset      : list of transactions, preprocessed using 'preprocess()'\n",
    "Ck           : list of itemsets to compute support for. \n",
    "min_support  : minimum support. Itemsets with support below this threshold\n",
    "               will be pruned.\n",
    "              \n",
    "output       : list of remaining itemsets, after the pruning step.\n",
    "support_dict : dictionary containing the support value for each itemset.\n",
    "\"\"\"\n",
    "def get_support(dataset, Ck, min_support):\n",
    "    \n",
    "    # This dictionary should contain the number of appearance of each itemset in the dataset.\n",
    "    # Itemset in Ck are represented as frozensets and can directly be uses as dictionary keys.\n",
    "    support_count = {}\n",
    "    \n",
    "    for transaction in dataset:\n",
    "        for candidate in Ck:\n",
    "            if candidate.issubset(transaction):\n",
    "                ...\n",
    "    \n",
    "    output = []\n",
    "    support_dict = {}\n",
    "    for key in support_count:\n",
    "        \n",
    "        support = ...\n",
    "        \n",
    "        if support >= min_support:\n",
    "            output.insert(0,key)\n",
    "            support_dict[key] = support\n",
    "    return output, support_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = [ l.strip().split(',') for i,l in enumerate(open('groceries.csv').readlines())]\n",
    "\n",
    "L,support = apriori(dataset,min_support=0.01)\n",
    "print_support(support,10,min_items=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "\n",
    "You can check the results of your implementation using MLXtend. Just run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori as mx_apriori\n",
    "\n",
    "df_dummy = pd.get_dummies(pd.Series(dataset).apply(pd.Series).stack()).sum(level=0)\n",
    "frequent_itemsets = mx_apriori(df_dummy, min_support=0.01, use_colnames=True)\n",
    "print_support_mx(frequent_itemsets,10,min_items=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 2: Association Rule Learning\n",
    "Such associations are not necessarily symmetric. Therefore, in the second part, we will use [association rule learning](https://en.wikipedia.org/wiki/Association_rule_learning) to better understand the directionality of our computed frequent itemsets. In other terms, we will have to infer if the purchase of one item generally implies the the purchase of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "L              : itemsets\n",
    "supportData    : dictionary storing itemsets support\n",
    "min_confidence : rules with a confidence under this threshold should be pruned\n",
    "\"\"\"\n",
    "def generate_rules(L, supportData, min_confidence=0.7):  \n",
    "    # Rules to be computed\n",
    "    rules = []\n",
    "    \n",
    "    # Iterate over itemsets of length 2..N\n",
    "    for i in range(1, len(L)):\n",
    "        \n",
    "        # Iterate over each frequent itemset\n",
    "        for freqSet in L[i]:\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            \n",
    "            # If the itemset contains more than 2 elements\n",
    "            # recursively generate candidates \n",
    "            if (i+1 > 2):\n",
    "                rules_from_consequent(freqSet, H1, supportData, rules, min_confidence)\n",
    "                compute_confidence(freqSet, H1, supportData, rules, min_confidence)\n",
    "            # If the itemsset contains 2 or less elements\n",
    "            # conpute rule confidence\n",
    "            else:\n",
    "                compute_confidence(freqSet, H1, supportData, rules, min_confidence)\n",
    "\n",
    "    return rules   \n",
    "\n",
    "\"\"\"\n",
    "freqSet        : frequent itemset\n",
    "H              : candidate elements to create a rule\n",
    "supportData    : dictionary storing itemsets support\n",
    "rules          : array to store rules\n",
    "min_confidence : rules with a confidence under this threshold should be pruned\n",
    "\"\"\"\n",
    "def rules_from_consequent(freqSet, H, supportData, rules, min_confidence=0.7):\n",
    "    m = len(H[0])\n",
    "    if (len(freqSet) > (m + 1)): \n",
    "\n",
    "        # create new candidates of size n+1\n",
    "        Hmp1 = generate_candidates(H)\n",
    "        Hmp1 = compute_confidence(freqSet, Hmp1, supportData, rules, min_confidence)\n",
    "        \n",
    "        if (len(Hmp1) > 1):    #need at least two sets to merge\n",
    "            rules_from_consequent(freqSet, Hmp1, supportData, rules, min_confidence)\n",
    "            \n",
    "\"\"\"\n",
    "Print the resulting rules\n",
    "\"\"\"\n",
    "def print_rules(rules,max_display=10):\n",
    "    print('confidence\\t rule')\n",
    "    print('-'*30)\n",
    "    for a,b,sup in sorted(rules, key=lambda x: x[2],reverse=True)[:max_display]:\n",
    "        print(\"%.2f\" % sup,'\\t',set(a),'->',set(b))\n",
    "def print_rules_mx(df,max_display=10):\n",
    "    print('confidence\\t rule')\n",
    "    print('-'*30)\n",
    "    df  = df.sort_values('confidence',ascending=False).iloc[:max_display]\n",
    "    for i,row in df.iterrows():\n",
    "        print(\"%.2f\" % float(row['confidence']),'\\t',set(row['antecedents']),'->',set(row['consequents']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "You will have to complete the method `compute_confidence(...)`, that computes confidence for a set of candidate rules H and prunes the rules that have a confidence below the specified threshold. Please complete it by computing rules confidence using the following formula:\n",
    "\n",
    "$$\\mathrm{conf}(X \\Rightarrow Y) = \\mathrm{supp}(X \\cup Y) / \\mathrm{supp}(X)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute confidence for a given set of rules and their respective support\n",
    "\n",
    "freqSet        : frequent itemset of N-element\n",
    "H              : list of candidate elements Y1, Y2... that are part of the frequent itemset\n",
    "supportData    : dictionary storing itemsets support\n",
    "rules          : array to store rules\n",
    "min_confidence : rules with a confidence under this threshold should be pruned\n",
    "\"\"\"\n",
    "def compute_confidence(freqSet, H, supportData, rules, min_confidence=0.7):\n",
    "    prunedH = [] \n",
    "    \n",
    "    for Y in H:\n",
    "        # Compute X which is the frequent itemset minus the considered Y\n",
    "        X           = ...\n",
    "        \n",
    "        # Compute support for both terms\n",
    "        support_XuY = ...\n",
    "        support_X   = ...\n",
    "        \n",
    "        # Compute confidence\n",
    "        conf        = ...\n",
    "        \n",
    "        if conf >= min_confidence: \n",
    "            rules.append((X, Y, conf))\n",
    "            prunedH.append(Y)\n",
    "    return prunedH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rules=generate_rules(L,support, min_confidence=0.1)\n",
    "print_rules(rules,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "\n",
    "You can check the results of your implementation using MLXtend. Just run the cell below (you will have to run the checking code of question 1 first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules as mx_association_rules\n",
    "\n",
    "rules_mx = mx_association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "print_rules_mx(rules_mx,max_display=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPFL Twitter Data\n",
    "\n",
    "Now that we have a working implementation, we will apply the Apriori algorithm on a dataset that you should know pretty well by now: EPFL Twitter data. In that scenario, tweets will be considered as transactions and words will be items. Let's see what kind of frequent associations we can discover.\n",
    "\n",
    "The method below cleans the tweets and formats them in the same format as the transactions of the previous exercise. Run the cells and generate the results for both algorithms. What can you observe from the association rules results? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of libraries and documents\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize, stem a document\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Remove stop words\n",
    "def clean_voc(documents):\n",
    "    cleaned = []\n",
    "    for tweet in documents:\n",
    "        new_tweet = []\n",
    "        tweet = tokenize(tweet).split()\n",
    "        for word in tweet:\n",
    "            if (word not in stopwords.words('english') and \n",
    "                word not in stopwords.words('german') and\n",
    "                word not in stopwords.words('french')):\n",
    "                if word==\"epflen\":\n",
    "                    word = \"epfl\"\n",
    "                new_tweet.append(word)\n",
    "        if len(new_tweet)>0:\n",
    "            cleaned.append(new_tweet)\n",
    "    return cleaned\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = clean_voc(original_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L,support = apriori(documents,min_support = 0.01)\n",
    "print_support(support,20,min_items=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rules=generate_rules(L,support, min_confidence=0.1)\n",
    "print_rules(rules,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 3: Pen and Paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given the following accident and weather data. Each line corresponds to one event:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. car_accident rain lightning wind clouds fire\n",
    "2. fire clouds rain lightning wind\n",
    "3. car_accident fire wind\n",
    "4. clouds rain wind\n",
    "5. lightning fire rain clouds  \n",
    "6. clouds wind car_accident  \n",
    "7. rain lightning clouds fire  \n",
    "8. lightning fire car_accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) You would like to know what is the likely cause of all the car accidents. What association rules do you need to look for? Compute the confidence and support values for these rules. Looking at these values, which is the most likely cause of the car accidents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Find all the association rules for minimal support 0.6 and minimal confidence of 1.0 (certainty). Follow the apriori algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
