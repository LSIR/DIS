{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Query Expansion and Indexing\n",
    "\n",
    "The following code is modified from Exercise 1. It is used to construct the vocabulary and vectorize the documents and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/elmas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "#with open(\"bread.txt\") as f:\n",
    "with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "    \n",
    "def search_vec(query, k):\n",
    "    query_vector = vectorize_query(query, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    ans = []\n",
    "    indices = []\n",
    "    for i in range(min(k,len(original_documents))):\n",
    "        ans.append(original_documents[scores[i][1]])\n",
    "        indices.append(scores[i][1])\n",
    "    return ans, indices, query_vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Relevance Feedback\n",
    "\n",
    "In this exercise we will implement and test Rocchio's method for user relevance feedback.\n",
    "\n",
    "Let the set of relevant documents to be $D_r $ and the set of non-relevant documents to be $D_{nr}$. Then the modified query  $\\vec{q_m}$  according to the Rocchio method is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{q_m} = \\alpha \\vec{q_0} + \\frac{\\beta}{|D_r|} \\sum_{\\vec{d_j} \\in D_r} \\vec{d_j} - \\frac{\\gamma}{|D_{nr}|} \\sum_{\\vec{d_j} \\in D_{nr}} \\vec{d_j}\n",
    "\\end{equation}\n",
    "In the Rocchio algorithm negative term weights are ignored. This means, for the negative term weights in $\\vec{q_m}$, we set them to be 0.\n",
    "\n",
    "First, complete the implementation of the Rocchio relevance feedback method, by adding the missing code for the function $expand\\_query$.   \n",
    "\n",
    "Then, compare the result obtained with the new query with the unmodified one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, alpha, beta, gamma):\n",
    "    # Note: relevant_doc_vecs and non_relevant_doc_vecs are list of vectors, vectors are also lists in this case. \n",
    "    # We are using (zip(*list)) to columnwise addition. i.e. [[1,2,3], [4,5,6]] iterate over tuples (1,4),(2,5),(3,6)\n",
    "    # Check here: https://stackoverflow.com/questions/29139350/difference-between-ziplist-and-ziplist\n",
    "    # You can use numpy if you want to go fancier\n",
    "    \n",
    "    num_rel = len(relevant_doc_vecs)\n",
    "    num_non_rel = len(non_relevant_doc_vecs)\n",
    "    \n",
    "    # Compute the first term in the Rocchio equation\n",
    "    norm_query_vector = query_vector*alpha\n",
    "    \n",
    "    # Compute the second term in the Rocchio equation\n",
    "    norm_sum_relevant = [beta*sum(x)/num_rel for x in zip(*relevant_doc_vecs)]\n",
    "    \n",
    "    # Compute the last term in the Rocchio equation\n",
    "    norm_sum_non_relevant = [-gamma*sum(x)/num_non_rel for x in zip(*non_relevant_doc_vecs)]\n",
    "    \n",
    "    # Sum all the terms\n",
    "    modified_query_vector = [sum(x) for x in zip(norm_sum_relevant, norm_sum_non_relevant, norm_query_vector)]\n",
    "    \n",
    "    # Ignore negative elements\n",
    "    modified_query_vector = [x if x>0 else 0 for x in modified_query_vector]\n",
    "    return modified_query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ\n",
      "1 New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD\n",
      "2 An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj\n",
      "3 New at @epfl_en Life Sciences @epflSV: \"From PhD directly to Independent Group Leader\" #ELFIR_EPFL:  Early Independence Research Scholars. See https://t.co/evqyqD7FFl, also for computational biology #compbio https://t.co/e3pDCg6NVb Deadline April 1 2018 at https://t.co/mJqcrfIqkb\n",
      "4 Video of Nicola Marzari from @EPFL_en  on Computational Discovery in the 21st Century during #PASC17 now online: https://t.co/tfCkEvYKtq https://t.co/httPdHcK9W\n"
     ]
    }
   ],
   "source": [
    "ans, result_doc_ids, query_vector = search_vec(\"computer science\", 5)\n",
    "for i in range(len(ans)):\n",
    "    print(i,ans[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified query:  20162017 amp barth biophys combin comput congrat control distanc epfl eth excit httpstcoarslxzoshq httpstcob9tglxo4kd httpstcoijwbaebocj httpstcoznjk3bz6mo interview model new news no1 no8 patrick professor protein rank scienc show subject univers vdtech via world\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ',\n",
       " 'New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD',\n",
       " 'An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj',\n",
       " 'International ranking: #ETH and #EPFL are top institutes',\n",
       " 'Interview (in French) de Patrick Aebischer, un \"innovation slasher\" @EPFL_en https://t.co/BtzhxEAEmN',\n",
       " 'The proteins that domesticated our genomes https://t.co/npGbUKJhBl  via @EPFL_en #VDtech https://t.co/It0SBqlKQc',\n",
       " \"New software can model natural light from the occupants' perspective https://t.co/RbMmN3Po5v via @EPFL_en #VDtech https://t.co/50enZtwUHi\",\n",
       " \"New software can model natural light from the occupants' perspective https://t.co/RbMmN3Po5v via @EPFL_en #VDtech https://t.co/lLIAvntc9R\",\n",
       " 'Latest work in our lab shows how feedback enhances brainwave control of a novel hand-exoskeleton https://t.co/VVPdX19fIM #epfl',\n",
       " '“Artificial intelligence has the potential to revolutionize transportation\" interview of Alexandre Alahi, new professor of Transportation Engineering @EPFL https://t.co/S8ZKRIAHbq RT @Trace_EPFL https://t.co/PQqkRY5ny9']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of indices marked as relevant\n",
    "# suppose first three documents were relevant and the rest were irrelevant.\n",
    "relevant_indices = [0,1,2]\n",
    "non_relevant_indices = [i for i in range(3, len(ans))]\n",
    "\n",
    "relevant_doc_ids = [result_doc_ids[i] for i in relevant_indices]\n",
    "non_relevant_doc_ids = [result_doc_ids[i] for i in non_relevant_indices]\n",
    "\n",
    "relevant_doc_vecs = [document_vectors[i] for i in relevant_doc_ids]\n",
    "non_relevant_doc_vecs = [document_vectors[i] for i in non_relevant_doc_ids]\n",
    "\n",
    "expanded_query = expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, 1, 1, 1)\n",
    "\n",
    "new_query = ' '.join([vocabulary[i] for i, val in enumerate(expanded_query) if val>0])\n",
    "\n",
    "new_ans , not_important_now, idontcare_anymore = search_vec(new_query, 10)\n",
    "\n",
    "print('Modified query: ', new_query)\n",
    "new_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Link based ranking\n",
    "\n",
    "### Preliminaries\n",
    "If you want to normalize a vector to L1-norm or L2-norm, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1-norm of [1 2 3] is [0.16666667 0.33333333 0.5       ]\n",
      "L2-norm of [1 2 3] is [0.26726124 0.53452248 0.80178373]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "\n",
    "pr = np.array([1,2,3])\n",
    "print(\"L1-norm of {0} is {1}\".format(pr, pr / np.linalg.norm(pr,1)))\n",
    "print(\"L2-norm of {0} is {1}\".format(pr, pr / np.linalg.norm(pr,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Page Rank (Eigen-vector method)\n",
    "Consider a tiny Web with three pages A, B and C with no inlinks,\n",
    "and with initial PageRank = 1. Initially, none of the pages link to\n",
    "any other pages and none link to them. \n",
    "Answer the following questions, and calculate the PageRank for\n",
    "each question.\n",
    "\n",
    "1. Link page A to page B.\n",
    "2. Link all pages to each other.\n",
    "3. Link page A to both B and C, and link pages B and C to A.\n",
    "4. Use the previous links and add a link from page C to page B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints:\n",
    "We are using the theoretical PageRank computation (without source of rank). See slide \"Transition Matrix for Random Walker\" in the lecture note. Columns of link matrix are from-vertex, rows of link matrix are to-vertex. We take the eigenvector with the largest eigenvalue.\n",
    "We only care about final ranking of the probability vector. You can choose the normalization (or not) of your choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Rmatrix(L):\n",
    "#     R = np.multiply(L, 1 / np.sum(L,axis=0)) # Use this matrix multiplication for faster running time if no column is zero\n",
    "    X = np.sum(L,axis=0)\n",
    "    n_nodes = L.shape[0]\n",
    "    R = np.zeros((n_nodes, n_nodes))\n",
    "    for i in range(L.shape[0]):\n",
    "        for j in range(L.shape[1]):\n",
    "            R[i,j] = L[i,j] / X[0,j] if X[0,j] != 0 else 0\n",
    "            \n",
    "#     R = np.multiply(L,R)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some time we might want to compute R outside the function to avoid recomputing large matrix\"\"\"\n",
    "def pagerank_eigen(L, R=None):\n",
    "#   Construct transition probability matrix from L\n",
    "    if R is None: R = create_Rmatrix(L)\n",
    "#     Compute eigen-vectors and eigen-values of R\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(R)\n",
    "#     Take the eigen-vector with maximum eigven-value\n",
    "    p = eigenvectors[:,np.argmax(np.absolute(eigenvalues))]\n",
    "    return (R,p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=[[0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 0]]\n",
      "R=[[0.  0.5 0.5]\n",
      " [0.5 0.  0.5]\n",
      " [0.5 0.5 0. ]]\n",
      "p=[0.57735027 0.57735027 0.57735027]\n"
     ]
    }
   ],
   "source": [
    "# Test with the question, e.g.\n",
    "L = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,1], \n",
    "    [1,1,0]\n",
    "])\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L={0}\\nR={1}\\np={2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Link page A to page B\n",
    "\n",
    "Result:\n",
    "$\n",
    "L =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    " ,\n",
    "R =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "  ,\n",
    "  \\vec{p} =\n",
    "  \\begin{bmatrix}\n",
    "\t0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "  \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=[[0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]]\n",
      "R=[[0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "p=[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "L = np.matrix([\n",
    "    [0,0,0], \n",
    "    [1,0,0], \n",
    "    [0,0,0]\n",
    "])\n",
    "\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L={0}\\nR={1}\\np={2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Link all pages to each other\n",
    "\n",
    "Result:\n",
    "$\n",
    "L =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    " ,\n",
    "R =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & \\frac{1}{2} & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2} & 0 & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "  ,\n",
    "  \\vec{p} =\n",
    "  \\begin{bmatrix}\n",
    "\t0.577 \\\\\n",
    "0.577 \\\\\n",
    "0.577 \\\\\n",
    "  \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=[[0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 0]]\n",
      "R=[[0.  0.5 0.5]\n",
      " [0.5 0.  0.5]\n",
      " [0.5 0.5 0. ]]\n",
      "p=[0.57735027 0.57735027 0.57735027]\n"
     ]
    }
   ],
   "source": [
    "L = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,1], \n",
    "    [1,1,0]\n",
    "])\n",
    "\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L={0}\\nR={1}\\np={2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Link page A to both B and C, and link pages B and C to A.\n",
    "\n",
    "Result:\n",
    "$\n",
    "L =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    " ,\n",
    "R =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 \\\\\n",
    "\\frac{1}{2} & 0 & 0 \\\\\n",
    "\\frac{1}{2} & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "  ,\n",
    "  \\vec{p} =\n",
    "  \\begin{bmatrix}\n",
    "\t0.816 \\\\\n",
    "0.408 \\\\\n",
    "0.408 \\\\\n",
    "  \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=[[0 1 1]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "R=[[0.  1.  1. ]\n",
      " [0.5 0.  0. ]\n",
      " [0.5 0.  0. ]]\n",
      "p=[0.81649658 0.40824829 0.40824829]\n"
     ]
    }
   ],
   "source": [
    "L = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,0], \n",
    "    [1,0,0]\n",
    "])\n",
    "\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L={0}\\nR={1}\\np={2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Use the previous links and add a link from page C to page B\n",
    "\n",
    "Result:\n",
    "$\n",
    "L =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    " ,\n",
    "R =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2} & 0 & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2} & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "  ,\n",
    "  \\vec{p} =\n",
    "  \\begin{bmatrix}\n",
    "\t0.743 \\\\\n",
    "0.557 \\\\\n",
    "0.371 \\\\\n",
    "  \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=[[0 1 1]\n",
      " [1 0 1]\n",
      " [1 0 0]]\n",
      "R=[[0.  1.  0.5]\n",
      " [0.5 0.  0.5]\n",
      " [0.5 0.  0. ]]\n",
      "p=[-0.74278135+0.j -0.55708601+0.j -0.37139068+0.j]\n"
     ]
    }
   ],
   "source": [
    "L = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,1], \n",
    "    [1,0,0]\n",
    "])\n",
    "\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L={0}\\nR={1}\\np={2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Page Rank (Iterative method)\n",
    "\n",
    "The eigen-vector method has some numerical issues (when computing eigen-vector) and not scalable with large datasets.\n",
    "\n",
    "We will apply the iterative method in the slide \"Practical Computation of PageRank\" of the lecture.\n",
    "\n",
    "Dataset for practice: https://snap.stanford.edu/data/ca-GrQc.html. It is available within the same folder of this github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_iterative(L, R=None):\n",
    "    if R is None: #We might want to compute R outside this function to avoid recomputing large matrix\n",
    "        R = np.multiply(L, 1 / np.sum(L,axis=0))\n",
    "        \n",
    "    N = R.shape[0]\n",
    "    e = np.ones(shape=(N,1))\n",
    "    q = 0.9\n",
    "\n",
    "    p = e\n",
    "    delta = 1\n",
    "    epsilon = 0.001\n",
    "    i = 0\n",
    "    while delta > epsilon:\n",
    "        p_prev = p\n",
    "        p = np.matmul(q * R, p_prev)\n",
    "        p = p + (1-q) / N * e\n",
    "        delta = np.linalg.norm(p-p_prev,1)\n",
    "        i += 1\n",
    "\n",
    "    print(\"Converged after {0} iterations\".format(i))\n",
    "    return R,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct link matrix from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5242\n",
      "[3466, 937, 5233]\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 0\n",
    "nodes_idx = dict() #Since the nodeIDs are not from 0 to N we need to build an index of nodes\n",
    "nodes = [] #We also want to store nodeIDs to return the result of ranking vector\n",
    "\n",
    "# Read the nodes\n",
    "with open(\"ca-GrQc.txt\") as f:\n",
    "    for line in f:\n",
    "        if '#' not in line:\n",
    "            source = int(line.split()[0])\n",
    "            target = int(line.split()[1])\n",
    "            if source not in nodes_idx.keys():\n",
    "                nodes_idx[source] = n_nodes\n",
    "                nodes.append(source)\n",
    "                n_nodes += 1\n",
    "            if target not in nodes_idx.keys():\n",
    "                nodes_idx[target] = n_nodes\n",
    "                nodes.append(target)\n",
    "                n_nodes += 1\n",
    "print(n_nodes)\n",
    "print(nodes[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "L = np.zeros((n_nodes, n_nodes))\n",
    "# Read the edges\n",
    "with open(\"ca-GrQc.txt\") as f:\n",
    "    for line in f:\n",
    "        if \"#\" not in line:\n",
    "            source = int(line.split()[0])\n",
    "            target = int(line.split()[1])\n",
    "            L[nodes_idx[target], nodes_idx[source]] = 1 #Columns of link matrix are from-vertices\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute transition probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.2   0.5   ... 0.    0.    0.   ]\n",
      " [0.125 0.    0.    ... 0.    0.    0.   ]\n",
      " [0.125 0.    0.    ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.    0.    ... 0.    0.5   0.5  ]\n",
      " [0.    0.    0.    ... 0.5   0.    0.5  ]\n",
      " [0.    0.    0.    ... 0.5   0.5   0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Here I use matrix multiplication from numpy for faster running time\n",
    "R = np.multiply(L, 1 / np.sum(L,axis=0))\n",
    "# R = create_Rmatrix(L) # This is much slower\n",
    "print(R)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will see that eigen-vector method is slow and has some numerical issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 88.81494784355164 seconds ---\n",
      "Ranking vector: p=[0.01151345+0.j 0.00719591+0.j 0.00287836+0.j ... 0.        +0.j\n",
      " 0.        +0.j 0.        +0.j]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "R,p = pagerank_eigen(L, R)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Ranking vector: p={0}\".format(p))\n",
    "# print(eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 128 iterations\n",
      "--- 16.894638061523438 seconds ---\n",
      "Ranking vector: p=[2.91315639e-04 1.88382754e-04 8.39741651e-05 ... 1.92156702e-04\n",
      " 1.92156702e-04 1.92156702e-04]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "R, p = pagerank_iterative(L,R)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Ranking vector: p={0}\".format(p[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract top-k nodes from the ranking vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 nodes: [14265 13801 13929]\n",
      "Their scores: [0.00144951 0.00141553 0.00138011]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array(p[:,0])\n",
    "k = 3\n",
    "k_idx = arr.argsort()[-k:][::-1]\n",
    "print(\"Top-{0} nodes: {1}\".format(k, np.array(nodes)[k_idx]))\n",
    "print(\"Their scores: {0}\".format(arr[k_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - Ranking Methodology (Hard)\n",
    "\n",
    "1. Give a directed graph, as small as possible, satisfying all the properties mentioned below:\n",
    "\n",
    "    1. There exists a path from node i to node j for all nodes i,j in the directed\n",
    "graph. Recall, with this property the jump to an arbitrary node in PageRank\n",
    "is not required, so that you can set q = 1 (refer lecture slides).\n",
    "\n",
    "    2. HITS authority ranking and PageRank ranking of the graph nodes are different.\n",
    "\n",
    "2. Give intuition/methodology on how you constructed such a directed graph with\n",
    "the properties described in (a).\n",
    "\n",
    "3. Are there specific graph structures with arbitrarily large instances where PageRank\n",
    "ranking and HITS authority ranking are the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions in pdf.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 - Hub and Authority\n",
    "\n",
    "### a)\n",
    "\n",
    "Let the adjacency matrix for a graph of four vertices ($n_1$ to $n_4$) be\n",
    "as follows:\n",
    "\n",
    "$\n",
    "A =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 & 1  \\\\\n",
    "\t0 & 0 & 1 & 1 \\\\\n",
    "\t1 & 0 & 0 & 1 \\\\\n",
    "\t0 & 0 & 0 & 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Calculate the authority and hub scores for this graph using the\n",
    "HITS algorithm with k = 6, and identify the best authority and\n",
    "hub nodes.\n",
    "\n",
    "### b)\n",
    "Apply the HITS algorithm to the dataset: https://snap.stanford.edu/data/ca-GrQc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** We follow the slide \"HITS algorithm\" in the lecture. **Denote $x$ as authority vector and $y$ as hub vector**. You can use matrix multiplication for the update steps in the slide \"Convergence of HITS\". Note that rows of adjacency matrix is from-vertex and columns of adjacency matrix is to-vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_iterative(A, k = 10):\n",
    "    N = A.shape[0]\n",
    "    x0, y0 = 1 / (N*N) * np.ones(N), 1 / (N*N) * np.ones(N) \n",
    "    xprev, yprev = x0, y0\n",
    "    delta1 = delta2 = 1\n",
    "    epsilon = 0.001 # We can strictly check for convergence rate of HITS algorithm\n",
    "    l = 0\n",
    "    while l < k and delta1 > epsilon and delta2 > epsilon:\n",
    "        y = np.matmul(A, xprev)\n",
    "        x = np.matmul(np.transpose(A), y) \n",
    "        x = x / np.linalg.norm(x,2)\n",
    "        y = y / np.linalg.norm(y,2)\n",
    "        delta1 = np.linalg.norm(x-xprev,1)\n",
    "        delta2 = np.linalg.norm(y-yprev,1)\n",
    "        xprev = x\n",
    "        yprev = y\n",
    "        l += 1\n",
    "    \n",
    "    print(\"Ran a total of {0} iterations with the convergence rate delta1, delta2={1},{2}\".format(l, delta1, delta2))\n",
    "    return xprev, yprev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran a total of 9 iterations with the convergence rate delta1, delta2=0.0007655728389590333,0.0016260291592009035\n",
      "Result using iterative method:\n",
      " Authoriy vector x=[0.16878931 0.27236456 0.49773665 0.80596894]\n",
      " Hub vector y=[0.65546933 0.54214151 0.40516824 0.33508393]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([\n",
    "    [0, 1, 1, 1], \n",
    "    [0, 0, 1, 1], \n",
    "    [1, 0, 0, 1],\n",
    "    [0, 0, 0, 1],\n",
    "])\n",
    "\n",
    "x, y = hits_iterative(A, 100)\n",
    "print(\"Result using iterative method:\\n Authoriy vector x={0}\\n Hub vector y={1}\".format(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details:**\n",
    "+ Initialization: \n",
    "  \n",
    "  $x_0 = \\frac{1}{4^2}(1,1,1,1) = ( 0.0625,  0.0625,  0.0625,  0.0625)$\n",
    "  \n",
    "  $y_0 = \\frac{1}{4^2}(1,1,1,1) = ( 0.0625,  0.0625,  0.0625,  0.0625)$\n",
    "  \n",
    "+ $k=1$:\n",
    "  \n",
    "  $x_1 = \\frac{A^t y_0}{||A^t y_0||} = (0.21320072,  0.21320072,  0.42640143,  0.85280287)$\n",
    "  \n",
    "  $y_1 = \\frac{A x_0}{|| A x_0 ||} = (0.70710678,  0.47140452,  0.47140452,  0.23570226)$\n",
    "  \n",
    "+ ...:\n",
    "  \n",
    "+ $k=6$:\n",
    "  \n",
    "   $x_6 = \\frac{A^t y_5}{||A^t y_5||} = (0.16887796,  0.27257494,  0.49774555, 0.80587375)$\n",
    "  \n",
    "  $y_6 = \\frac{A x_5}{||A x_5||} = (0.65357971,  0.54153747,  0.40815386,  0.33612671)$\n",
    "  \n",
    "\n",
    "**Conclusion:**\n",
    "+ Best authority node: $n_4$. Best hub node: $n_1$.\n",
    " \n",
    "**Check with the theoretical result (convergence condition):**\n",
    "  \n",
    "+ $x^*$ is the principal eigenvector (i.e. with largest eigenvalue) of $A^t A$: $(0.16845787,  0.27257056,  0.49801119,  0.80579904)$\n",
    "  \n",
    "+ $y^*$ is the principal eigenvector (i.e. with largest eigenvalue) of $A A^t$: $(0.65549599,  0.54215478,  0.4051188,   0.33507008)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "We reuse the link matrix $L$ to compute the adjacency matrix $A$ of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.transpose(L)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run HITS algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran a total of 39 iterations with the convergence rate delta1, delta2=0.0009846048386951208,0.000984604838696764\n",
      "Result using iterative method:\n",
      " Authoriy vector x=[3.48948810e-05 1.45965389e-05 6.68131703e-06 ... 1.28245551e-54\n",
      " 1.28245551e-54 1.28245551e-54]\n",
      " Hub vector y=[3.48948810e-05 1.45965389e-05 6.68131703e-06 ... 1.28245551e-54\n",
      " 1.28245551e-54 1.28245551e-54]\n"
     ]
    }
   ],
   "source": [
    "x, y = hits_iterative(A, 100)\n",
    "print(\"Result using iterative method:\\n Authoriy vector x={0}\\n Hub vector y={1}\".format(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can see that the two authority vector and hub vector are the same. So the network must be an undirected graph**\n",
    "\n",
    "**Interpret the result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 authorities: [21012  2741 12365]\n",
      "Their scores: [0.15556238 0.15357488 0.15307255]\n",
      "Top-3 hubs: [21012  2741 12365]\n",
      "Their scores: [0.15556238 0.15357488 0.15307255]\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "arr = np.array(x)\n",
    "k_idx = arr.argsort()[-k:][::-1]\n",
    "print(\"Top-{0} authorities: {1}\".format(k, np.array(nodes)[k_idx]))\n",
    "print(\"Their scores: {0}\".format(arr[k_idx]))\n",
    "\n",
    "arr = np.array(y)\n",
    "k_idx = arr.argsort()[-k:][::-1]\n",
    "print(\"Top-{0} hubs: {1}\".format(k, np.array(nodes)[k_idx]))\n",
    "print(\"Their scores: {0}\".format(arr[k_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can also use linear algebra property of HITS to compute the result (slide \"Convergence of HITS\"):**.\n",
    "  \n",
    "+ $x^*$ is the principal eigenvector (i.e. with largest eigenvalue) of $A^t A$\n",
    "+ $y^*$ is the principal eigenvector (i.e. with largest eigenvalue) of $A A^t$\n",
    "\n",
    "However, the computation will be much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result using linear algebra:\n",
      " Authoriy vector x=[3.48948725e-05+0.j 1.45965100e-05+0.j 6.68130767e-06+0.j ...\n",
      " 0.00000000e+00+0.j 0.00000000e+00+0.j 0.00000000e+00+0.j]\n",
      " Hub vector y=[3.48948725e-05+0.j 1.45965100e-05+0.j 6.68130767e-06+0.j ...\n",
      " 0.00000000e+00+0.j 0.00000000e+00+0.j 0.00000000e+00+0.j]\n"
     ]
    }
   ],
   "source": [
    "xstar_ev, xstar = np.linalg.eig(np.matmul(np.transpose(A),A))\n",
    "ystar_ev, ystar = np.linalg.eig(np.matmul(A,np.transpose(A)))\n",
    "xstar, ystar = xstar[:,np.argmax(np.absolute(xstar_ev))], ystar[:,np.argmax(np.absolute(ystar_ev))]\n",
    "# ystar = -xstar if all(xstar<0) else xstar\n",
    "# ystar = -ystar if all(ystar<0) else ystar\n",
    "print(\"Result using linear algebra:\\n Authoriy vector x={0}\\n Hub vector y={1}\".format(xstar, ystar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
