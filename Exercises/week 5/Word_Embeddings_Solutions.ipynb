{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link based ranking\n",
    "\n",
    "**Preliminaries:** If you want to normalize a vector to L1-norm or L2-norm, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "\n",
    "pr = np.array([1,2,3])\n",
    "print(\"L1-norm of {0} is {1}\".format(pr, pr / np.linalg.norm(pr,1)))\n",
    "print(\"L2-norm of {0} is {1}\".format(pr, pr / np.linalg.norm(pr,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank (Eigen-vector method)\n",
    "Consider a tiny Web with three pages A, B and C with no inlinks,\n",
    "and with initial PageRank = 1. Initially, none of the pages link to\n",
    "any other pages and none link to them. \n",
    "Answer the following questions, and calculate the PageRank for\n",
    "each question.\n",
    "\n",
    "1. Link page A to page B.\n",
    "2. Link all pages to each other.\n",
    "3. Link page A to both B and C, and link pages B and C to A.\n",
    "4. Use the previous links and add a link from page C to page B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "We are using the theoretical PageRank computation (without source of rank). See slide \"Transition Matrix for Random Walker\" in the lecture note. Columns of link matrix are from-vertex, rows of link matrix are to-vertex. We take the eigenvector with the largest eigenvalue.\n",
    "We only care about final ranking of the probability vector. You can choose the normalization (or not) of your choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Rmatrix(L):\n",
    "#     R = np.multiply(L, 1 / np.sum(L,axis=0)) # Use this matrix multiplication for faster running time if no column is zero\n",
    "    X = np.sum(L,axis=0)\n",
    "    n_nodes = L.shape[0]\n",
    "    R = np.zeros((n_nodes, n_nodes))\n",
    "    for i in range(L.shape[0]):\n",
    "        for j in range(L.shape[1]):\n",
    "            R[i,j] = L[i,j] / X[0,j] if X[0,j] != 0 else 0\n",
    "            \n",
    "#     R = np.multiply(L,R)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some time we might want to compute R outside the function to avoid recomputing large matrix\"\"\"\n",
    "def pagerank_eigen(L, R=None):\n",
    "#   Construct transition probability matrix from L\n",
    "    if R is None: R = create_Rmatrix(L)\n",
    "#     Compute eigen-vectors and eigen-values of R\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(R)\n",
    "#     Take the eigen-vector with maximum eigven-value\n",
    "    p = eigenvectors[:,np.argmax(np.absolute(eigenvalues))]\n",
    "    return (R,p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the question, e.g.\n",
    "L = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,1], \n",
    "    [1,1,0]\n",
    "])\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L={0}\\nR={1}\\np={2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Link page A to page B\n",
    "\n",
    "Result:\n",
    "$\n",
    "L =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    " ,\n",
    "R =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "  ,\n",
    "  \\vec{p} =\n",
    "  \\begin{bmatrix}\n",
    "\t0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "  \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.matrix([\n",
    "    [0,0,0], \n",
    "    [1,0,0], \n",
    "    [0,0,0]\n",
    "])\n",
    "\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L={0}\\nR={1}\\np={2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Link all pages to each other\n",
    "\n",
    "Result:\n",
    "$\n",
    "L =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    " ,\n",
    "R =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & \\frac{1}{2} & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2} & 0 & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "  ,\n",
    "  \\vec{p} =\n",
    "  \\begin{bmatrix}\n",
    "\t0.577 \\\\\n",
    "0.577 \\\\\n",
    "0.577 \\\\\n",
    "  \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,1], \n",
    "    [1,1,0]\n",
    "])\n",
    "\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L={0}\\nR={1}\\np={2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Link page A to both B and C, and link pages B and C to A.\n",
    "\n",
    "Result:\n",
    "$\n",
    "L =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    " ,\n",
    "R =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 \\\\\n",
    "\\frac{1}{2} & 0 & 0 \\\\\n",
    "\\frac{1}{2} & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "  ,\n",
    "  \\vec{p} =\n",
    "  \\begin{bmatrix}\n",
    "\t0.816 \\\\\n",
    "0.408 \\\\\n",
    "0.408 \\\\\n",
    "  \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,0], \n",
    "    [1,0,0]\n",
    "])\n",
    "\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L={0}\\nR={1}\\np={2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Use the previous links and add a link from page C to page B\n",
    "\n",
    "Result:\n",
    "$\n",
    "L =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    " ,\n",
    "R =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2} & 0 & \\frac{1}{2} \\\\\n",
    "\\frac{1}{2} & 0 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "  ,\n",
    "  \\vec{p} =\n",
    "  \\begin{bmatrix}\n",
    "\t0.743 \\\\\n",
    "0.557 \\\\\n",
    "0.371 \\\\\n",
    "  \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,1], \n",
    "    [1,0,0]\n",
    "])\n",
    "\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L={0}\\nR={1}\\np={2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank (Iterative method)\n",
    "\n",
    "The eigen-vector method has some numerical issues (when computing eigen-vector) and not scalable with large datasets.\n",
    "\n",
    "We will apply the iterative method in the slide \"Practical Computation of PageRank\" of the lecture.\n",
    "\n",
    "Dataset for practice: https://snap.stanford.edu/data/ca-GrQc.html. It is available within the same folder of this github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_iterative(L, R=None):\n",
    "    if R is None: #We might want to compute R outside this function to avoid recomputing large matrix\n",
    "        R = np.multiply(L, 1 / np.sum(L,axis=0))\n",
    "        \n",
    "    N = R.shape[0]\n",
    "    e = np.ones(shape=(N,1))\n",
    "    q = 0.9\n",
    "\n",
    "    p = e\n",
    "    delta = 1\n",
    "    epsilon = 0.001\n",
    "    i = 0\n",
    "    while delta > epsilon:\n",
    "        p_prev = p\n",
    "        p = np.matmul(q * R, p_prev)\n",
    "        p = p + (1-q) / N * e\n",
    "        delta = np.linalg.norm(p-p_prev,1)\n",
    "        i += 1\n",
    "\n",
    "    print(\"Converged after {0} iterations\".format(i))\n",
    "    return R,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct link matrix from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 0\n",
    "nodes_idx = dict() #Since the nodeIDs are not from 0 to N we need to build an index of nodes\n",
    "nodes = [] #We also want to store nodeIDs to return the result of ranking vector\n",
    "\n",
    "# Read the nodes\n",
    "with open(\"ca-GrQc.txt\") as f:\n",
    "    for line in f:\n",
    "        if '#' not in line:\n",
    "            source = int(line.split()[0])\n",
    "            target = int(line.split()[1])\n",
    "            if source not in nodes_idx.keys():\n",
    "                nodes_idx[source] = n_nodes\n",
    "                nodes.append(source)\n",
    "                n_nodes += 1\n",
    "            if target not in nodes_idx.keys():\n",
    "                nodes_idx[target] = n_nodes\n",
    "                nodes.append(target)\n",
    "                n_nodes += 1\n",
    "print(n_nodes)\n",
    "print(nodes[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.zeros((n_nodes, n_nodes))\n",
    "# Read the edges\n",
    "with open(\"ca-GrQc.txt\") as f:\n",
    "    for line in f:\n",
    "        if \"#\" not in line:\n",
    "            source = int(line.split()[0])\n",
    "            target = int(line.split()[1])\n",
    "            L[nodes_idx[target], nodes_idx[source]] = 1 #Columns of link matrix are from-vertices\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute transition probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I use matrix multiplication from numpy for faster running time\n",
    "R = np.multiply(L, 1 / np.sum(L,axis=0))\n",
    "# R = create_Rmatrix(L) # This is much slower\n",
    "print(R)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will see that eigen-vector method is slow and has some numerical issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "R,p = pagerank_eigen(L, R)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Ranking vector: p={0}\".format(p))\n",
    "# print(eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "R, p = pagerank_iterative(L,R)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Ranking vector: p={0}\".format(p[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract top-k nodes from the ranking vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(p[:,0])\n",
    "k = 3\n",
    "k_idx = arr.argsort()[-k:][::-1]\n",
    "print(\"Top-{0} nodes: {1}\".format(k, np.array(nodes)[k_idx]))\n",
    "print(\"Their scores: {0}\".format(arr[k_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Methodology (Hard)\n",
    "\n",
    "1. Give a directed graph, as small as possible, satisfying all the properties mentioned below:\n",
    "\n",
    "    1. There exists a path from node i to node j for all nodes i,j in the directed\n",
    "graph. Recall, with this property the jump to an arbitrary node in PageRank\n",
    "is not required, so that you can set q = 1 (refer lecture slides).\n",
    "\n",
    "    2. HITS authority ranking and PageRank ranking of the graph nodes are different.\n",
    "\n",
    "2. Give intuition/methodology on how you constructed such a directed graph with\n",
    "the properties described in (a).\n",
    "\n",
    "3. Are there specific graph structures with arbitrarily large instances where PageRank\n",
    "ranking and HITS authority ranking are the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions in pdf.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hub and Authority\n",
    "\n",
    "### a)\n",
    "\n",
    "Let the adjacency matrix for a graph of four vertices ($n_1$ to $n_4$) be\n",
    "as follows:\n",
    "\n",
    "$\n",
    "A =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 & 1  \\\\\n",
    "\t0 & 0 & 1 & 1 \\\\\n",
    "\t1 & 0 & 0 & 1 \\\\\n",
    "\t0 & 0 & 0 & 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Calculate the authority and hub scores for this graph using the\n",
    "HITS algorithm with k = 6, and identify the best authority and\n",
    "hub nodes.\n",
    "\n",
    "### b)\n",
    "Apply the HITS algorithm to the dataset: https://snap.stanford.edu/data/ca-GrQc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** We follow the slide \"HITS algorithm\" in the lecture. **Denote $x$ as authority vector and $y$ as hub vector**. You can use matrix multiplication for the update steps in the slide \"Convergence of HITS\". Note that rows of adjacency matrix is from-vertex and columns of adjacency matrix is to-vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_iterative(A, k = 10):\n",
    "    N = A.shape[0]\n",
    "    x0, y0 = 1 / (N*N) * np.ones(N), 1 / (N*N) * np.ones(N) \n",
    "    xprev, yprev = x0, y0\n",
    "    delta1 = delta2 = 1\n",
    "    epsilon = 0.001 # We can strictly check for convergence rate of HITS algorithm\n",
    "    l = 0\n",
    "    while l < k and delta1 > epsilon and delta2 > epsilon:\n",
    "        y = np.matmul(A, xprev)\n",
    "        x = np.matmul(np.transpose(A), y) \n",
    "        x = x / np.linalg.norm(x,2)\n",
    "        y = y / np.linalg.norm(y,2)\n",
    "        delta1 = np.linalg.norm(x-xprev,1)\n",
    "        delta2 = np.linalg.norm(y-yprev,1)\n",
    "        xprev = x\n",
    "        yprev = y\n",
    "        l += 1\n",
    "    \n",
    "    print(\"Ran a total of {0} iterations with the convergence rate delta1, delta2={1},{2}\".format(l, delta1, delta2))\n",
    "    return xprev, yprev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([\n",
    "    [0, 1, 1, 1], \n",
    "    [0, 0, 1, 1], \n",
    "    [1, 0, 0, 1],\n",
    "    [0, 0, 0, 1],\n",
    "])\n",
    "\n",
    "x, y = hits_iterative(A, 100)\n",
    "print(\"Result using iterative method:\\n Authoriy vector x={0}\\n Hub vector y={1}\".format(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details:**\n",
    "+ Initialization: \n",
    "  \n",
    "  $x_0 = \\frac{1}{4^2}(1,1,1,1) = ( 0.0625,  0.0625,  0.0625,  0.0625)$\n",
    "  \n",
    "  $y_0 = \\frac{1}{4^2}(1,1,1,1) = ( 0.0625,  0.0625,  0.0625,  0.0625)$\n",
    "  \n",
    "+ $k=1$:\n",
    "  \n",
    "  $x_1 = \\frac{A^t y_0}{||A^t y_0||} = (0.21320072,  0.21320072,  0.42640143,  0.85280287)$\n",
    "  \n",
    "  $y_1 = \\frac{A x_0}{|| A x_0 ||} = (0.70710678,  0.47140452,  0.47140452,  0.23570226)$\n",
    "  \n",
    "+ ...:\n",
    "  \n",
    "+ $k=6$:\n",
    "  \n",
    "   $x_6 = \\frac{A^t y_5}{||A^t y_5||} = (0.16887796,  0.27257494,  0.49774555, 0.80587375)$\n",
    "  \n",
    "  $y_6 = \\frac{A x_5}{||A x_5||} = (0.65357971,  0.54153747,  0.40815386,  0.33612671)$\n",
    "  \n",
    "\n",
    "**Conclusion:**\n",
    "+ Best authority node: $n_4$. Best hub node: $n_1$.\n",
    " \n",
    "**Check with the theoretical result (convergence condition):**\n",
    "  \n",
    "+ $x^*$ is the principal eigenvector (i.e. with largest eigenvalue) of $A^t A$: $(0.16845787,  0.27257056,  0.49801119,  0.80579904)$\n",
    "  \n",
    "+ $y^*$ is the principal eigenvector (i.e. with largest eigenvalue) of $A A^t$: $(0.65549599,  0.54215478,  0.4051188,   0.33507008)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "We reuse the link matrix $L$ to compute the adjacency matrix $A$ of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.transpose(L)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run HITS algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = hits_iterative(A, 100)\n",
    "print(\"Result using iterative method:\\n Authoriy vector x={0}\\n Hub vector y={1}\".format(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can see that the two authority vector and hub vector are the same. So the network must be an undirected graph**\n",
    "\n",
    "**Interpret the result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "arr = np.array(x)\n",
    "k_idx = arr.argsort()[-k:][::-1]\n",
    "print(\"Top-{0} authorities: {1}\".format(k, np.array(nodes)[k_idx]))\n",
    "print(\"Their scores: {0}\".format(arr[k_idx]))\n",
    "\n",
    "arr = np.array(y)\n",
    "k_idx = arr.argsort()[-k:][::-1]\n",
    "print(\"Top-{0} hubs: {1}\".format(k, np.array(nodes)[k_idx]))\n",
    "print(\"Their scores: {0}\".format(arr[k_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can also use linear algebra property of HITS to compute the result (slide \"Convergence of HITS\"):**.\n",
    "  \n",
    "+ $x^*$ is the principal eigenvector (i.e. with largest eigenvalue) of $A^t A$\n",
    "+ $y^*$ is the principal eigenvector (i.e. with largest eigenvalue) of $A A^t$\n",
    "\n",
    "However, the computation will be much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstar_ev, xstar = np.linalg.eig(np.matmul(np.transpose(A),A))\n",
    "ystar_ev, ystar = np.linalg.eig(np.matmul(A,np.transpose(A)))\n",
    "xstar, ystar = xstar[:,np.argmax(np.absolute(xstar_ev))], ystar[:,np.argmax(np.absolute(ystar_ev))]\n",
    "# ystar = -xstar if all(xstar<0) else xstar\n",
    "# ystar = -ystar if all(ystar<0) else ystar\n",
    "print(\"Result using linear algebra:\\n Authoriy vector x={0}\\n Hub vector y={1}\".format(xstar, ystar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introducing Word Embeddings\n",
    "\n",
    "In this exercise, we would train word embeddings using a state-of-the-art embeddings library fastText. The first step of the exercise is to install the fasttext library. Proceed with the following steps:\n",
    "\n",
    "## FastText installation\n",
    "\n",
    "> pip install fasttext\n",
    "\n",
    "If you are having problems, try this command:\n",
    "> sudo apt-get install g++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised('epfldocs.txt', model = 'cbow')\n",
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pretrained Embeddings\n",
    "If you are unable to install fasttext, you can use the preembeddings we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "# Edit this, or just move model_epfldocs.vec to the directory where this notebook is situated\n",
    "directory_path = './'\n",
    "\n",
    "def load_embeddings(file_name):\n",
    "    with codecs.open(file_name, 'r', 'utf-8') as f_in:\n",
    "        lines = f_in.readlines()\n",
    "        lines = lines[1:]\n",
    "        vocabulary, wv = zip(*[line.strip().split(' ', 1) for line in lines])\n",
    "    wv = np.loadtxt(wv)\n",
    "    return wv, vocabulary\n",
    "\n",
    "\n",
    "# Replace the path based on your own machine.\n",
    "word_embeddings, vocabulary = load_embeddings(directory_path + 'model_epfldocs.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embeddings\n",
    "\n",
    "In the third phase of this exercise, we will visualize the generated embeddings using t-SNE (T-Distributed Stochastic Neighbouring Entities). t-SNE is a dimensionality reduction algorithm which is well suited for such visualization tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=1000, init = 'pca') # Changed the parameters for provided in the exercise for less KL divergence and better readibility\n",
    "vis_data = tsne.fit_transform(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_data_x = vis_data[:,0]\n",
    "vis_data_y = vis_data[:,1]\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plt.figure(figsize=(40, 40)) \n",
    "plt.scatter(vis_data_x, vis_data_y)\n",
    "\n",
    "for label, x, y in zip(vocabulary, vis_data_x, vis_data_y):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Observe the plot of word embeddings. Do you observe any patterns?\n",
    "\n",
    "<b> You will not encounter a very obvious pattern (because the data is small and too much EPFL oriented) but some small patterns. </b>\n",
    "\n",
    "Words are close to their context words i.e. Professor and MartinVetterli, President and MartinVetterli, Register and conference.\n",
    "\n",
    "Similar words are clustered together: conference, workshop, discussion.\n",
    "\n",
    "Stop words are not clustered together with other words because they cooccur with many words.\n",
    "\n",
    "Artikels are close to each other (Une, du, des)\n",
    "\n",
    "## Question\n",
    "\n",
    "Write a python function to find the most similar terms for a given term. The similarity between two terms is defined as the cosine similarity between their corresponding word embeddings.\n",
    "\n",
    "Find the top 5 terms that are most similar to 'la', 'EPFL', '#robot', 'this', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(input_term, word_embeddings, vocabulary, num_terms=3):\n",
    "    term_embeddings_dict = {}\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        term_embeddings_dict[term] = word_embeddings[i]\n",
    "        \n",
    "    if input_term not in term_embeddings_dict:\n",
    "        return \"Term not in the vocabulary\"\n",
    "    \n",
    "    input_term_embedding = term_embeddings_dict[input_term]\n",
    "    term_similarities = []\n",
    "    for term, embedding in term_embeddings_dict.items():\n",
    "        term_similarities.append([term, cosine_similarity(input_term_embedding.reshape((1,-1)), \n",
    "                                                          embedding.reshape((1,-1)))])\n",
    "        \n",
    "    sorted_terms = sorted(term_similarities, key = lambda x: -1 * x[1])[0:num_terms]\n",
    "    \n",
    "    return sorted_terms\n",
    "\n",
    "# Note:\n",
    "# cosine_similarity takes a matrix (of all word vectors) as input,\n",
    "# so it's better not to loop through all terms to computer term_similarities for efficieny reasons\n",
    "# I avoided doing this in this part for better readability, \n",
    "# although you will see matrix based solution in get_most_similar_documents\n",
    "# in any case, you will never need to compute similarities by yourself, instead you can use:\n",
    "# import gensim\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('model.vec')\n",
    "# similar = model.most_similar(positive=['epfl'],topn=10)\n",
    "\n",
    "find_most_similar('EPFL', word_embeddings, vocabulary, num_terms=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question [Optional]\n",
    "​\n",
    "Observe the word embeddings that are visualized in this link http://www.anthonygarvan.com/wordgalaxy/ . Can you make some interesting observations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Search Engine Using Word Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we would put our word embeddings to test by using them for information retrieval. \n",
    "The idea is that, the documents that have the most similar embedding vectors to the one belongs to query should rank higher.\n",
    "The documents may not necessarily include the keywords in the query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of libraries and documents\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"epfldocs.txt\", ) as f:\n",
    "    content = f.readlines()\n",
    "        \n",
    "original_documents = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Since both the documents and the query is of variable size, we should aggregate the vectors of the words in the query by some strategy. This could be taking the minimum vector, maximum vector or the mean. Fill in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of vectors for easier search\n",
    "vector_dict = dict(zip(vocabulary, word_embeddings))\n",
    "\n",
    "def aggregate_vector_list(vlist, aggfunc):\n",
    "    if aggfunc == 'max':\n",
    "        return np.array(vlist).max(axis=0)\n",
    "    elif aggfunc == 'min':\n",
    "        return np.array(vlist).min(axis=0)\n",
    "    elif aggfunc == 'mean':\n",
    "        return np.array(vlist).mean(axis=0)\n",
    "    else:\n",
    "        return np.zeros(np.array(vlist).shape[1])\n",
    "\n",
    "possible_aggfuncs = [\"max\", \"min\", \"mean\"]\n",
    "\n",
    "aggregated_doc_vectors = {}\n",
    "\n",
    "# Aggregate vectors of documents beforehand\n",
    "for aggfunc in possible_aggfuncs:\n",
    "    aggregated_doc_vectors[aggfunc] = np.zeros((len(original_documents), word_embeddings.shape[1]))\n",
    "    for index, doc in enumerate(original_documents):\n",
    "        vlist = [vector_dict[token] for token in fasttext.tokenize(doc) if token in vector_dict]\n",
    "        if(len(vlist) < 1):\n",
    "            continue \n",
    "        else:\n",
    "            aggregated_doc_vectors[aggfunc][index] = aggregate_vector_list(vlist, aggfunc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Aggregate the query and find the most similar documents using cosine distance between the query's vector and document's aggregated vector. Are they seem to relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = \"EPFL\"\n",
    "\n",
    "def aggregate_query(query, aggfunc):\n",
    "    tokens = fasttext.tokenize(query)\n",
    "    if(len(tokens) == 1):\n",
    "        if(tokens[0] in vocabulary):\n",
    "            return vector_dict[tokens[0]]\n",
    "    elif(len(tokens) > 1):\n",
    "        vlist = []\n",
    "        for token in tokens:\n",
    "            if (token in vocabulary):\n",
    "                vlist.append(vector_dict[token])\n",
    "        \n",
    "        return aggregate_vector_list(vlist, aggfunc)\n",
    "    else:\n",
    "        print(\"%s is not in the vocabulary.\" % (query))\n",
    "    \n",
    "def get_most_similar_documents(query_vector, aggfunc, k = 5):\n",
    "    query_vector = query_vector.reshape(1, -1)\n",
    "    # Calculate the similarity with each vector. \n",
    "    # Hint: Cosine similarity function takes a matrix as input so you do not need to loop through each document vector.\n",
    "    sim = cosine_similarity(query_vector, aggregated_doc_vectors[aggfunc])\n",
    "    \n",
    "    # Rank the document vectors according to their cosine similarity with \n",
    "    indexes = np.argsort(sim, axis=-1, kind='quicksort', order=None) # This is sorted in ascending order\n",
    "    indexes = indexes[0]\n",
    "    indexes = indexes[::-1] # Convert to descending\n",
    "    return indexes\n",
    "\n",
    "def search_vec_embeddings(query, topk = 10, aggfunc = 'mean'):\n",
    "    query_vector = aggregate_query(query, aggfunc)\n",
    "    indexes = get_most_similar_documents(query_vector, aggfunc)\n",
    "    # Print the top k documents\n",
    "    indexes = indexes[0:topk]\n",
    "    for index in indexes:\n",
    "        print(original_documents[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vec_embeddings('EPFL', aggfunc = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Compare the results with the vector space retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR SPACE RETRIEVAL (From Exercise 1)\n",
    "# Retrieval oracle \n",
    "from operator import itemgetter\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "\n",
    "# Return all document ids that that have cosine similarity with the query larger than a threshold\n",
    "def search_vec_sklearn(query, topk = 10, features = features, threshold=0.1):\n",
    "    new_features = tf.transform([query])\n",
    "    print(new_features.shape)\n",
    "    cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold or i >= topk:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    \n",
    "    for index in doc_ids:\n",
    "        print(original_documents[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vec_embeddings('EPFL', aggfunc = 'mean')\n",
    "print(\"---------------------------------\")\n",
    "search_vec_sklearn(\"EPFL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "You will realize that not all the words in your queries are in the vocabulary, so your queries fail to retrieve any documents. Think of possible solutions to overcome this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use a bigger corpus\n",
    "2. Try pretrained vectors (check glove)\n",
    "3. Preprocess (lowercase, stem) (if the former 2 options are not available)\n",
    "4. Use heuristics to find similar words and aggregate their vectors, i.e. get a word in the query's stem and the words that have the same stem, get the words with close edit distance to catch hashtags and mentions etc.\n",
    "    Some examples:\n",
    "    * MAGA -> What it stands for: Make America Great Again (What MAGA stands for)\n",
    "    * MAGA -> Cooccuring words, i.e. TCOT (Top Conservatives on Twitter, likely precedes the slogan MAGA)\n",
    "    * MAGA -> words of concept (Republican) = {Tcot, PJNET, Republic etc.} use the concept vector / average of the word vectors' belonging to that concept\n",
    "    Use wordnet (https://wordnet.princeton.edu) but no MAGA unfortunately :(\n",
    "5. Search the literature for more ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
