{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solutions are presented at: https://tube.switch.ch/videos/33053e02#19:16\n",
    "\n",
    "# Introducing Word Embeddings\n",
    "\n",
    "In this exercise, we would train word embeddings using a state-of-the-art embeddings library fastText. The first step of the exercise is to install the fasttext library. Proceed with the following steps:\n",
    "\n",
    "## FastText installation\n",
    "\n",
    "> pip install fasttext\n",
    "\n",
    "If you are having problems, try this command:\n",
    "> sudo apt-get install g++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised('epfldocs.txt', model = 'cbow')\n",
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pretrained Embeddings\n",
    "If you are unable to install fasttext, you can use the preembeddings we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "# Edit this, or just move model_epfldocs.vec to the directory where this notebook is situated\n",
    "directory_path = './'\n",
    "\n",
    "def load_embeddings(file_name):\n",
    "    with codecs.open(file_name, 'r', 'utf-8') as f_in:\n",
    "        lines = f_in.readlines()\n",
    "        lines = lines[1:]\n",
    "        vocabulary, wv = zip(*[line.strip().split(' ', 1) for line in lines])\n",
    "    wv = np.loadtxt(wv)\n",
    "    return wv, vocabulary\n",
    "\n",
    "\n",
    "# Replace the path based on your own machine.\n",
    "word_embeddings, vocabulary = load_embeddings(directory_path + 'model_epfldocs.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embeddings\n",
    "\n",
    "In the third phase of this exercise, we will visualize the generated embeddings using t-SNE (T-Distributed Stochastic Neighbouring Entities). t-SNE is a dimensionality reduction algorithm which is well suited for such visualization tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=1000, init = 'pca') # Changed the parameters for provided in the exercise for less KL divergence and better readibility\n",
    "vis_data = tsne.fit_transform(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_data_x = vis_data[:,0]\n",
    "vis_data_y = vis_data[:,1]\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plt.figure(figsize=(40, 40)) \n",
    "plt.scatter(vis_data_x, vis_data_y)\n",
    "\n",
    "for label, x, y in zip(vocabulary, vis_data_x, vis_data_y):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Observe the plot of word embeddings. Do you observe any patterns?\n",
    "\n",
    "<b> You will not encounter a very obvious pattern (because the data is small and too much EPFL oriented) but some small patterns. </b>\n",
    "\n",
    "Words are close to their context words i.e. Professor and MartinVetterli, President and MartinVetterli, Register and conference.\n",
    "\n",
    "Similar words are clustered together: conference, workshop, discussion.\n",
    "\n",
    "Stop words are not clustered together with other words because they cooccur with many words.\n",
    "\n",
    "Artikels are close to each other (Une, du, des)\n",
    "\n",
    "## Question\n",
    "\n",
    "Write a python function to find the most similar terms for a given term. The similarity between two terms is defined as the cosine similarity between their corresponding word embeddings.\n",
    "\n",
    "Find the top 5 terms that are most similar to 'la', 'EPFL', '#robot', 'this', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(input_term, word_embeddings, vocabulary, num_terms=3):\n",
    "    term_embeddings_dict = {}\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        term_embeddings_dict[term] = word_embeddings[i]\n",
    "        \n",
    "    if input_term not in term_embeddings_dict:\n",
    "        return \"Term not in the vocabulary\"\n",
    "    \n",
    "    input_term_embedding = term_embeddings_dict[input_term]\n",
    "    term_similarities = []\n",
    "    for term, embedding in term_embeddings_dict.items():\n",
    "        term_similarities.append([term, cosine_similarity(input_term_embedding.reshape((1,-1)), \n",
    "                                                          embedding.reshape((1,-1)))])\n",
    "        \n",
    "    sorted_terms = sorted(term_similarities, key = lambda x: -1 * x[1])[0:num_terms]\n",
    "    \n",
    "    return sorted_terms\n",
    "\n",
    "# Note:\n",
    "# cosine_similarity takes a matrix (of all word vectors) as input,\n",
    "# so it's better not to loop through all terms to computer term_similarities for efficieny reasons\n",
    "# I avoided doing this in this part for better readability, \n",
    "# although you will see matrix based solution in get_most_similar_documents\n",
    "# in any case, you will never need to compute similarities by yourself, instead you can use:\n",
    "# import gensim\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('model.vec')\n",
    "# similar = model.most_similar(positive=['epfl'],topn=10)\n",
    "\n",
    "find_most_similar('EPFL', word_embeddings, vocabulary, num_terms=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question [Optional]\n",
    "​\n",
    "Observe the word embeddings that are visualized in this link http://www.anthonygarvan.com/wordgalaxy/ . Can you make some interesting observations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Search Engine Using Word Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we would put our word embeddings to test by using them for information retrieval. \n",
    "The idea is that, the documents that have the most similar embedding vectors to the one belongs to query should rank higher.\n",
    "The documents may not necessarily include the keywords in the query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of libraries and documents\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"epfldocs.txt\", ) as f:\n",
    "    content = f.readlines()\n",
    "        \n",
    "original_documents = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Since both the documents and the query is of variable size, we should aggregate the vectors of the words in the query by some strategy. This could be taking the minimum vector, maximum vector or the mean. Fill in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of vectors for easier search\n",
    "vector_dict = dict(zip(vocabulary, word_embeddings))\n",
    "\n",
    "def aggregate_vector_list(vlist, aggfunc):\n",
    "    if aggfunc == 'max':\n",
    "        return np.array(vlist).max(axis=0)\n",
    "    elif aggfunc == 'min':\n",
    "        return np.array(vlist).min(axis=0)\n",
    "    elif aggfunc == 'mean':\n",
    "        return np.array(vlist).mean(axis=0)\n",
    "    else:\n",
    "        return np.zeros(np.array(vlist).shape[1])\n",
    "\n",
    "possible_aggfuncs = [\"max\", \"min\", \"mean\"]\n",
    "\n",
    "aggregated_doc_vectors = {}\n",
    "\n",
    "# Aggregate vectors of documents beforehand\n",
    "for aggfunc in possible_aggfuncs:\n",
    "    aggregated_doc_vectors[aggfunc] = np.zeros((len(original_documents), word_embeddings.shape[1]))\n",
    "    for index, doc in enumerate(original_documents):\n",
    "        vlist = [vector_dict[token] for token in fasttext.tokenize(doc) if token in vector_dict]\n",
    "        if(len(vlist) < 1):\n",
    "            continue \n",
    "        else:\n",
    "            aggregated_doc_vectors[aggfunc][index] = aggregate_vector_list(vlist, aggfunc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Aggregate the query and find the most similar documents using cosine distance between the query's vector and document's aggregated vector. Are they seem to relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = \"EPFL\"\n",
    "\n",
    "def aggregate_query(query, aggfunc):\n",
    "    tokens = fasttext.tokenize(query)\n",
    "    if(len(tokens) == 1):\n",
    "        if(tokens[0] in vocabulary):\n",
    "            return vector_dict[tokens[0]]\n",
    "    elif(len(tokens) > 1):\n",
    "        vlist = []\n",
    "        for token in tokens:\n",
    "            if (token in vocabulary):\n",
    "                vlist.append(vector_dict[token])\n",
    "        \n",
    "        return aggregate_vector_list(vlist, aggfunc)\n",
    "    else:\n",
    "        print(\"%s is not in the vocabulary.\" % (query))\n",
    "    \n",
    "def get_most_similar_documents(query_vector, aggfunc, k = 5):\n",
    "    query_vector = query_vector.reshape(1, -1)\n",
    "    # Calculate the similarity with each vector. \n",
    "    # Hint: Cosine similarity function takes a matrix as input so you do not need to loop through each document vector.\n",
    "    sim = cosine_similarity(query_vector, aggregated_doc_vectors[aggfunc])\n",
    "    \n",
    "    # Rank the document vectors according to their cosine similarity with \n",
    "    indexes = np.argsort(sim, axis=-1, kind='quicksort', order=None) # This is sorted in ascending order\n",
    "    indexes = indexes[0]\n",
    "    indexes = indexes[::-1] # Convert to descending\n",
    "    return indexes\n",
    "\n",
    "def search_vec_embeddings(query, topk = 10, aggfunc = 'mean'):\n",
    "    query_vector = aggregate_query(query, aggfunc)\n",
    "    indexes = get_most_similar_documents(query_vector, aggfunc)\n",
    "    # Print the top k documents\n",
    "    indexes = indexes[0:topk]\n",
    "    for index in indexes:\n",
    "        print(original_documents[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vec_embeddings('EPFL', aggfunc = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Compare the results with the vector space retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR SPACE RETRIEVAL (From Exercise 1)\n",
    "# Retrieval oracle \n",
    "from operator import itemgetter\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "\n",
    "# Return all document ids that that have cosine similarity with the query larger than a threshold\n",
    "def search_vec_sklearn(query, topk = 10, features = features, threshold=0.1):\n",
    "    new_features = tf.transform([query])\n",
    "    print(new_features.shape)\n",
    "    cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold or i >= topk:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    \n",
    "    for index in doc_ids:\n",
    "        print(original_documents[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vec_embeddings('EPFL', aggfunc = 'mean')\n",
    "print(\"---------------------------------\")\n",
    "search_vec_sklearn(\"EPFL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "You will realize that not all the words in your queries are in the vocabulary, so your queries fail to retrieve any documents. Think of possible solutions to overcome this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use a bigger corpus\n",
    "2. Try pretrained vectors (check glove)\n",
    "3. Preprocess (lowercase, stem) (if the former 2 options are not available)\n",
    "4. Use heuristics to find similar words and aggregate their vectors, i.e. get a word in the query's stem and the words that have the same stem, get the words with close edit distance to catch hashtags and mentions etc.\n",
    "    Some examples:\n",
    "    * MAGA -> What it stands for: Make America Great Again (What MAGA stands for)\n",
    "    * MAGA -> Cooccuring words, i.e. TCOT (Top Conservatives on Twitter, likely precedes the slogan MAGA)\n",
    "    * MAGA -> words of concept (Republican) = {Tcot, PJNET, Republic etc.} use the concept vector / average of the word vectors' belonging to that concept\n",
    "    Use wordnet (https://wordnet.princeton.edu) but no MAGA unfortunately :(\n",
    "5. Search the literature for more ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
