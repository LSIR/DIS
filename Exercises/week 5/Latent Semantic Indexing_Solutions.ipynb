{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Exercise: Latent Semantic Indexing\n",
    "In this exercise we would learn about Latent Semantic Indexing (LSI) based retrieval models.\n",
    "\n",
    "### Goal:\n",
    "- Implement a search engine using LSI\n",
    "- Visualize the LSI concepts\n",
    "- Compare the retrieval results with scikit vector space retrieval method (as an oracle)\n",
    "\n",
    "### What you are learning in this exercise:\n",
    "1. Learn more about LSI approach\n",
    "2. How retrieval results can be different in LSI and vector space retrieval models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Implement Latent Semantic Indexing (LSI)\n",
    "### 1.1 Read the corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required libraries.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import math\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')).union(set(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    '''Reads corpus from files.'''\n",
    "    \n",
    "    documents = []\n",
    "    orig_docs = []\n",
    "    DIR = './'\n",
    "    tknzr = TweetTokenizer()\n",
    "    with open(\"epfldocs.txt\", encoding = \"utf-8\") as f:\n",
    "        content = f.readlines()\n",
    "    for text in content:\n",
    "        orig_docs.append(text)\n",
    "        # split into words\n",
    "        tokens = tknzr.tokenize(text)\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "\n",
    "        documents.append(' '.join(words))\n",
    "    return documents, orig_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, orig_docs = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Build the vocabulary by selecting top-k frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary_frequency(corpus, vocab_len):\n",
    "    '''Select top-k (k = vocab_len) words in term of frequencies as vocabulary'''\n",
    "    \n",
    "    count = {}\n",
    "    for document in corpus:\n",
    "        for word in document.split():\n",
    "            if word in count:\n",
    "                count[word] += 1\n",
    "            else:\n",
    "                count[word] = 1\n",
    "    \n",
    "    sorted_count_by_freq = sorted(count.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    vocabulary = [x[0] for x in sorted_count_by_freq[:vocab_len]]\n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_freq = create_vocabulary_frequency(documents, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Construct the term document matrix\n",
    "In this question, you need to construct the term document matrix given the vocabulary and the set of documents.\n",
    "The value of a cell (i, j) is the term frequency of the term i in document j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_term_document_matrix(vocabulary, documents):\n",
    "    # Construct term-doc matrix\n",
    "    matrix = np.zeros((len(vocabulary), len(documents)))\n",
    "    for j, document in enumerate(documents):\n",
    "        counter = Counter(document.split())\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            if word in counter:\n",
    "                matrix[i,j] = counter[word]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_matrix_freq = construct_term_document_matrix(vocab_freq, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Perform LSI by selecting the first 100 largest singular values of the term document matrix  \n",
    "Hint 1: np.linalg.svd(M, full_matrices=False) performs SVD on the matrix $\\mathbf{M}$ and returns $\\mathbf{K}, \\mathbf{S}, \\mathbf{D}^T$\n",
    "\n",
    " -  $\\mathbf{K}, \\mathbf{D}^T$ are matrices with orthonormal columns\n",
    " -  $\\mathbf{S}$ is a **vector** of singular values in a **descending** order\n",
    " \n",
    "Hint 2: np.diag(V) converts a vector to a diagonal matrix\n",
    "\n",
    "Hint 3: To select \n",
    " - The first k rows of a matrix A, use A[0:k, :]\n",
    " - The first k columns of a matrix A, use A[:, 0:k]\n",
    " - The submatrix from first k rows and k columns of a matrix A, use A[0:k, 0:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_svd(term_doc_matrix, num_val):\n",
    "    K, S, Dt = np.linalg.svd(term_doc_matrix, full_matrices=False)\n",
    "    K_sel = K[:,0:num_val]\n",
    "    S_sel = np.diag(S)[0:num_val,0:num_val]\n",
    "    Dt_sel = Dt[0:num_val,:]\n",
    "    return K_sel, S_sel, Dt_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_freq, S_freq, Dt_freq = truncated_svd(term_doc_matrix_freq,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Transform the given query\n",
    "In this question, you need to construct a vector representation for the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['epfl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_document_vector(query, vocabulary):\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for word in query:\n",
    "        try:\n",
    "            vector[vocabulary.index(word)] += 1\n",
    "        except: # if query word is not in vocabulary\n",
    "            # ignore it\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: \n",
    " -  To compute inverse of a matrix M, use np.linalg.inv(M)\n",
    " -  To compute the dot product of A, B, use np.dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_query_vector(query, vocabulary, K_sel, S_sel):\n",
    "    q = query_to_document_vector(query, vocabulary)\n",
    "    mapper = np.dot(K_sel, np.linalg.inv(S_sel))\n",
    "    q_trans =  np.dot( q, mapper)\n",
    "    return q_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector_freq = construct_query_vector(query, vocab_freq, K_freq, S_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Retrieve top-10 relevant documents\n",
    "In this question, you need to retrieve the top-10 documents that are relevant to the query using cosine similarity. You are given a function to compute the cosine simimlarity and a function that return the top-k documents given the query and document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy*1.0/math.sqrt(sumxx*sumyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query_vector, top_k, Dt_sel):\n",
    "    scores = [[cosine_similarity(query_vector, Dt_sel[:,d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    doc_ids = []\n",
    "    retrieved = []\n",
    "    for i in range(top_k):\n",
    "        doc_ids.append(scores[i][1])\n",
    "        retrieved.append(orig_docs[scores[i][1]])\n",
    "    return doc_ids, retrieved\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_ids_freq, retrieved_docs_freq = retrieve_documents(query_vector_freq, 10, Dt_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(retrieved_docs_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: Evaluation\n",
    "In this question, we consider the scikit reference code as an ‚Äúoracle‚Äù that supposedly gives the correct result. You need to compare your retrieval results with this oracle using the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval oracle \n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), vocabulary=vocab_freq, min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(documents)\n",
    "npm_tfidf = features.todense()\n",
    "\n",
    "# Return all document ids that that have cosine similarity with the query larger than a threshold\n",
    "def search_vec_sklearn(query, features, threshold=0.3):\n",
    "    new_features = tf.transform([query])\n",
    "    cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_ids = search_vec_sklearn(\" \".join(query), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in gt_ids:\n",
    "    print(orig_docs[i].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Compute F1-score at 10 between the oracle and your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(predict, gt, k):\n",
    "    correct_recall = set(predict[:k]).intersection(set(gt))\n",
    "    return len(correct_recall)/len(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_at_k(predict, gt, k):\n",
    "    correct_predict = set(predict[:k]).intersection(set(gt))\n",
    "    return len(correct_predict)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1score(predict, gt, k):\n",
    "    prec = compute_precision_at_k(predict, gt, k)\n",
    "    rec = compute_recall_at_k(predict, gt, k)\n",
    "    print(prec, rec)\n",
    "    return 2*prec*rec/(prec+rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1score(retrieved_ids_freq, gt_ids, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3. Visualization\n",
    "Plot the terms using two principal concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Compute the term vectors using two principal concepts\n",
    "Hint: you can reuse a method from the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_vecs_freq, _, _ = truncated_svd(term_doc_matrix_freq,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_vecs_freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: You can use plt.scatter(x, y) for a scatter plot. x is a vector of x-axis value and y is a vector of y-axis value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(term_vecs_freq[:, 0], term_vecs_freq[:, 1])\n",
    "for i, t in enumerate(vocab_freq):\n",
    "    plt.annotate(t, (term_vecs_freq[i, 0], term_vecs_freq[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Explain the scatter plot of the term vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "epfl and epflen are two Twitter handles of EPFL which explains why the two principal concepts are mainly made up of these terms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "75fd394e35225182f207b93437350142e41aafd8fa2b11cc1a17258e1fa2f196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
