{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Exercise 1\n",
    "In this exercise, we will understand the functioning of TF/IDF ranking and implement and test a vector space retrieval model.\n",
    "\n",
    "### Use case:\n",
    "You are an engineer who is building a search engine for the recipe finder application _bestbakesinthebuilding.com_. In this application, users can search for their favorite recipe, ingredients, or any recipe-relevant term. The search engine should return relevant recipes that are stored in the website's DB. For the implementation of the searching functionality, you have to use the vector space retrieval model.\n",
    "\n",
    "To build your system, you have available recipe data in the file bread.txt which is a dump from the application's DB.\n",
    "\n",
    "The schema of the data is the following:\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | How to Bake Breads Without Baking Recipes\n",
    "  2     | Smith Pies: Best Pies in London\n",
    "  3     | Numerical Recipes: The Art of Scientific Computing\n",
    "  4     | Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
    "  5     | Pastry: A Book of Best French Pastry Recipes\n",
    "\n",
    "\n",
    "### Goal:\n",
    "1. Preprocess the input text by tokenizing it and stemming it.\n",
    "2. Vectorize the preprocessed text by implementing the TF/IDF function.\n",
    "3. Test your method by searching for the query $Q = ``baking''$ and find the top-ranked documents according to the TF/IDF rank. \n",
    "4. Compare your implementation with the existing implementation of scikit-learn library.\n",
    "\n",
    "### What are you learning in this exercise:\n",
    "- Vector space retrieval model implementation.\n",
    "- TF/IDF ranking.\n",
    "- Evaluation of a text retrieval system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of libraries and documents\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preprocess the input text by tokenizing it and stemming it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() \n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    It tokenizes and stems an input text.\n",
    "    \n",
    "    :param text: str, with the input text\n",
    "    :return: list, of the tokenized and stemmed tokens.\n",
    "    \"\"\"\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d) for d in original_documents]\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Vectorize the preprocessed text by implementing the TF/IDF function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for the TF/IDF implementation\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = list(set([item for sublist in documents for item in sublist]))\n",
    "vocabulary.sort()\n",
    "\n",
    "def idf_values(vocabulary, documents):\n",
    "    \"\"\"\n",
    "    It computes IDF scores, storing idf values in a dictionary.\n",
    "    \n",
    "    :param vocabulary: list of str, with the unique tokens of the vocabulary.\n",
    "    :param documents: list of lists of str, with tokenized sentences.\n",
    "    :return: dict with the idf values for each vocabulary word.\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = # YOUR CODE HERE\n",
    "    return idf\n",
    "\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    \"\"\"\n",
    "    It generates the vector for an input document (with normalization).\n",
    "    \n",
    "    :param document: list of str with the tokenized documents.\n",
    "    :param vocabulary: list of str, with the unique tokens of the vocabulary.\n",
    "    :param idf: dict with the idf values for each vocabulary word.\n",
    "    :return: list of floats\n",
    "    \"\"\"\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] =  # YOUR CODE HERE\n",
    "    return vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
    "\n",
    "def cosine_similarity(v1,v2):\n",
    "    \"\"\"\n",
    "    It computes cosine similarity.\n",
    "    \n",
    "    :param v1: list of floats, with the vector of a document.\n",
    "    :param v2: list of floats, with the vector of a document.\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    \n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result =  # YOUR CODE HERE\n",
    "    return result\n",
    "\n",
    "def search_vec(query, topk):\n",
    "    \"\"\"\n",
    "    It computes the search result (get the topk documents).\n",
    "    \n",
    "    :param query: str\n",
    "    :param topk: int\n",
    "    \"\"\"\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    for i in range(topk):\n",
    "        print(original_documents[scores[i][1]])\n",
    "\n",
    "# HINTS\n",
    "# natural logarithm function\n",
    "#     math.log(n,math.e)\n",
    "# Function to count term frequencies in a document\n",
    "#     Counter(document)\n",
    "# most common elements for a list\n",
    "#     counts.most_common(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test your method by searching for the query  ùëÑ=‚Äò‚Äòùëèùëéùëòùëñùëõùëî‚Ä≥  and find the top-ranked documents according to the TF/IDF rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference code using scikit-learn\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "new_features = tf.transform(['baking'])\n",
    "\n",
    "cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "topk = 5\n",
    "for i in range(topk):\n",
    "    print(original_documents[related_docs_indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Compare your implementation with the existing implementation of scikit-learn library.\n",
    "\n",
    "In this part, we consider the scikit reference code as an ‚Äúoracle‚Äù that supposedly gives the correct result. Your task is to compare your implemented tf-idf retrieval model with this oracle for the following queries \"computer science\", \"IC school\", \"information systems\" on the **epfldocs.txt** collection.\n",
    "\n",
    "For this exercise, you need to replace **bread.txt** in the first cell with **epfldocs.txt** and rerun all the cells from the begining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval oracle \n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "\n",
    "def search_vec_sklearn(query, features, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Return all document ids that that have cosine similarity with the query larger than a threshold.\n",
    "    \n",
    "    :param query: str\n",
    "    :param features: ndarray\n",
    "    :param threshold: float\n",
    "    :return: list of int\n",
    "    \"\"\"\n",
    "    new_features = tf.transform([query])\n",
    "    cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_ids = search_vec_sklearn('computer science', features)\n",
    "for i, v in enumerate(ret_ids):\n",
    "    print(original_documents[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"computer science\", \"IC school\", \"information systems\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(predict, gt, k):\n",
    "    \"\"\"\n",
    "    It computes the recall score at a defined set of retrieved documents.\n",
    "    \n",
    "    :param predict: list of predictions\n",
    "    :param gt: list of actual data\n",
    "    :param k: int\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_at_k(predict, gt, k):\n",
    "    \"\"\"\n",
    "    It computes the precision score at a defined set of retrieved documents.\n",
    "    \n",
    "    :param predict: list of predictions\n",
    "    :param gt: list of actual data\n",
    "    :param k: int\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_map(queries, K = 10):\n",
    "    \"\"\"\n",
    "    It computes mean average precision (MAP) for a set of queries.\n",
    "    \n",
    "    :param queries: list of str\n",
    "    :param K: int\n",
    "    \"\"\"\n",
    "    map_score = 0\n",
    "    for i, query in enumerate(queries):\n",
    "        precision_for_query = 0\n",
    "        predict = search_vec(query, K)\n",
    "        gt = search_vec_sklearn(query, features)\n",
    "        for k in range(1, K+1):\n",
    "            # YOUR CODE HERE\n",
    "        map_score += # YOUR CODE HERE\n",
    "    map_score = # YOUR CODE HERE\n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_map(queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
