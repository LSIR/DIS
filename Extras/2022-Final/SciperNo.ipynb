{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò DIS Final Exam - Fall 2022\n",
    "\n",
    "**üéâ Welcome to DIS Final exam that takes place on the 1st of February 2023 from 15:00 to 18:00.**\n",
    "\n",
    "> Please fill the following info:\n",
    "> - Your Name: \n",
    "> - Your SCIPER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer all the parts of the exam:\n",
    "\n",
    "- [PART 0: Rename your notebook with your SciperNo](#part0)\n",
    "\n",
    "- [PART 1: Multiple Choice Questions - Quiz](#part1)\n",
    "\n",
    "- [PART 2: Theory questions](#part2)\n",
    "\n",
    "- [PART 3: Programming exercise](#part3)\n",
    "\n",
    "    - [3.1: Parse and understand the data](#part31)\n",
    "        - 3.1.1 Create the vocabulary of the documents\n",
    "\n",
    "    - [3.2: Encode documents with Vector Space Retrieval](#part32)\n",
    "        - 3.2.1 Build the term-frequency matrix.\n",
    "        - 3.2.2 Build the inverse document-frequency matrix\n",
    "        - 3.2.3 Vectorize input with Vector Space Model\n",
    "\n",
    "    - [3.3: k-Nearest-Neighbors (kNN)](#part33)\n",
    "        - 3.3.1 Implement kNN function\n",
    "        - 3.3.2 Print k=10 closests documents to the given query\n",
    "        - 3.3.3 Implement probabilistic and weigting estimation of kNN\n",
    "        - 3.3.4 Compute weighting and probabilistic estimation of the given query\n",
    "        - 3.3.5 Implement a Rocchio classifier\n",
    "        - 3.3.6 Compute Rocchio estimation of the given query\n",
    "\n",
    "    - [3.4: Naive Bayes Classifier](#part34)\n",
    "        - 3.4.1 Compute the Naive Bayes estimation for the given query\n",
    "        - 3.4.2 Discuss the difference the above classifers\n",
    "\n",
    "    - [3.5: Association rules](#part35)\n",
    "        - 3.5.1 Compute support and confidence\n",
    "        - 3.5.2 Compute lift\n",
    "        - 3.5.3 Explanation of implemented metrics\n",
    "        \n",
    "- [SUBMIT EXAM](#submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üçÄ GOOD LUCK üçÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0'></a>\n",
    "## PART 0: Rename your notebook with your SciperNo\n",
    "\n",
    "The final sumbitted file should have the following name: `SciperNo.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "## PART 1: [Multiple Choice Questions - Quiz](https://moodle.epfl.ch/mod/quiz/view.php?id=1235302)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "## PART 2: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given a document collection with a vocabulary consisting of three words, $V = {a,b,c}$, and two documents $d_1$ = aabc and $d_2 = abc$. The query is $q = ab$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **2.1. Using standard vector space retrieval, is it possible to enforce a ranking of <br> a) $d_1 > d_2$ <br> b) $d_2 > d_1$ <br> by adding suitable documents to the collection. If yes, give examples of such documents to be added, if no, provide an argument why this cannot be the case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è PLEASE WRITE YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **2.2. Using smoothed probabilistic retrieval (with $\\lambda=0.5$), is it possible to enforce a ranking <br> a)$d_1 > d_2$ <br>b)$d_2 > d_1$ <br> by adding suitable documents to the collection. If yes, give examples of such documents to be added, if no, provide an argument why this cannot be the case.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è PLEASE WRITE YOUR ANSWER HERE**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **2.3. Is it possible to enforce a ranking $d_2 > d_1$ with vector space retrieval and $d_1 > d_2$ with probabilistic retrieval ($\\lambda=0.5$), by adding the same documents to the collection? If yes, give examples of such documents to be added, if no, provide an argument why this cannot be the case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è PLEASE WRITE YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3'></a>\n",
    "## PART 3: Programming exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° THE BACKSTORY\n",
    "\n",
    "You are given a \"news topic classification\" dataset (`data.csv`), and the task is to apply document classification techniques you've learned during the semester. The goal is to classify news articles based on the topic they refer to.\n",
    "\n",
    "### üì∞ THE DATA\n",
    "\n",
    "This dataset contains ~7000 samples of news articles which consists of 3 columns:\n",
    "\n",
    "The first column is `label`, the second is `title` and the third is `description`.\n",
    "\n",
    "The topic labels are numbered 1-4 where `1` represents topic **\"World\"**, `2` represents topic **\"Sports\"**, `3` represents **\"Business\"** and `4` represents **\"Sci/Tech\"**.\n",
    "\n",
    "| Column     | Description                   |\n",
    "|------------|-------------------------------|\n",
    "| **label**  | The topic label/topic id of the article|\n",
    "| **title**  | The title of the article |\n",
    "| **description**  | The description of the article |\n",
    "\n",
    "\n",
    "### ‚úÖ THE TASK\n",
    "\n",
    "You need to build a KNN and a Naive Bayes classifer to classify the articles into the 4 different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part31'></a>\n",
    "### 3.1: Parse and understand the data\n",
    "\n",
    "*(1 sub-question)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries- you can additionally import any library you want.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "data_path = \"data.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "print(\"Number of news articles: \", len(data))\n",
    "\n",
    "# Plot the distribution of topics\n",
    "labels = data.label.unique()\n",
    "sizes = [Counter(data.label)[i] for i in labels]\n",
    "plt.figure( figsize=(5,5) )\n",
    "plt.pie(sizes ,  labels=labels , autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions - NOTHING TO CHANGE HERE\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words('english')])\n",
    "\n",
    "# Preprocess articles\n",
    "def preprocess_text(documents):\n",
    "    docs = list()\n",
    "    for doc in documents:\n",
    "        docs.append(tokenize(doc).split())  # tokenize\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize titles of the articles \n",
    "original_documents = [doc[\"title\"].strip() for _, doc in data.iterrows()]\n",
    "tokenized_documents = preprocess_text(original_documents)\n",
    "documents_topics = list(data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.1.1 Create the vocabulary of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_frequency(documents):\n",
    "    \"\"\"\n",
    "    It parses the input documents and creates a dictionary with the terms and term frequencies.\n",
    "    \n",
    "    INPUT:\n",
    "    Doc1: hello hello world\n",
    "    Doc2: hello friend\n",
    "    \n",
    "    OUTPUT:\n",
    "    {'hello': 3,\n",
    "    'world': 1,\n",
    "    'friend': 1}\n",
    "\n",
    "    :param documents: list of list of str, with the tokenized documents.\n",
    "    :return: dict, with keys the words and values the frequency of each word.\n",
    "    \"\"\"\n",
    "    vocabulary = dict()\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # --------------\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary\n",
    "vocabulary = get_vocabulary_frequency(tokenized_documents)\n",
    "f\"Vocabulary Size: {len(vocabulary)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the most frequent token\n",
    "voc_sorted_keys = sorted(vocabulary, key=vocabulary.get, reverse=True)\n",
    "print(f\"{voc_sorted_keys[0]} : {vocabulary[voc_sorted_keys[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part32'></a>\n",
    "### PART 3.2: Encode documents with Vector Space Retrieval\n",
    "\n",
    "*(3 sub-questions)*\n",
    "\n",
    "In this part, we will encode/vectorize the documents using the **Vector Space Model**. \n",
    "More specifically:\n",
    "- we will compute the term-frequency matrix **(tf)**\n",
    "- we will compute the inverse document frequency **(idf)**\n",
    "- we will vectorize/encode the articles with **tf-idf**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.2.1 Build the term-frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(documents, vocabulary):\n",
    "    \"\"\"\n",
    "    It creates the term-frequency matrix with rows the terms of the vocabulary and columns the number of documents.\n",
    "    Each value of the matrix represents the frequency (normalized to document max frequecy) of a term (row) \n",
    "    in a document (column).\n",
    "    Example:\n",
    "    \n",
    "    > INPUT:\n",
    "    documents:\n",
    "    Doc1: hello hello world\n",
    "    Doc2: hello friend\n",
    "    \n",
    "    voc: \n",
    "    [hello, world, friend]\n",
    "    \n",
    "    > OUPUT:    \n",
    "    [[1, 1],\n",
    "    [0.5, 0],\n",
    "    [0, 1]]\n",
    "    \n",
    "    :param documents: list of list of str, with the tokenized documents.\n",
    "    :param vocabulary: dict with the vocabulary (computed in 1.1) and each term's frequency.\n",
    "    :return: np.array with the document-term frequencies\n",
    "    \"\"\"\n",
    "    document_term_freq = np.zeros(shape=(len(vocabulary), len(documents)))\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # --------------\n",
    "    \n",
    "    return document_term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = get_tf(tokenized_documents, vocabulary)\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.2.2 Build the inverse document-frequency matrix (idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute IDF, storing idf values in a dictionary\n",
    "def get_idf(vocabulary, documents):\n",
    "    \"\"\"\n",
    "    It computes IDF scores, storing idf values in a dictionary.\n",
    "    \n",
    "    :param documents: list of list of str, with the tokenized tweets.\n",
    "    :param vocabulary: dict with the vocabulary (computed in 1.1) and each term's frequency.\n",
    "    :return: dict with the terms as keys and values the idf for each term.\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # --------------\n",
    "    return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = get_idf(vocabulary, tokenized_documents)\n",
    "len(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.2.3 Vectorization of input with the Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize_vsr(document, vocabulary, idf):\n",
    "    \"\"\"\n",
    "    It takes the input text and vectorizes it based on the tf-idf formula.\n",
    "    \n",
    "    :param document: list of str, with the tokenized document\n",
    "    :param vocabulary: dict, with the vocabulary (computed in 1.1) and each term's frequency.\n",
    "    :param idf: dict, with the terms as keys and values the idf for each term.\n",
    "    :return: np.array, with the vectorized document\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # --------------\n",
    "    return vector\n",
    "    \n",
    "vectorized_documents = np.array([vectorize_vsr(s, vocabulary, idf)  for i, s in enumerate(tokenized_documents)])\n",
    "vectorized_documents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part33'></a>\n",
    "### PART 3.3: k-Nearest-Neighbors (kNN)\n",
    "\n",
    "*(7 sub-questions)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> #### 3.3.1 Implement kNN function (finding k nearest documents for a given document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    It computes cosine similarity.\n",
    "    \n",
    "    :param v1: list of floats, with the vector of a document.\n",
    "    :param v2: list of floats, with the vector of a document.\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "        sim = 0\n",
    "    else:\n",
    "        sim = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return sim\n",
    "\n",
    "def euclidean_distance(v1, v2):\n",
    "    \"\"\" It computes the euclidean distance between to vectors.\n",
    "    :param v1: First vector (numpy array).\n",
    "    :param v2: Second vector (numpy array).\n",
    "    :return: Euclidean distance (float)\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(v1 - v2)\n",
    "    \n",
    "def knn(doc_vectors, query_vector, k=10):\n",
    "    \"\"\" It finds the `k` nearest documents to the given query (based on euclidean distance).\n",
    "    :param doc_vectors: An array of document vectors (np.array(np.array)).\n",
    "    :param query_vector: Query representation (np.array)\n",
    "    :return: List of document indices (list(int))\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # --------------\n",
    "    return top_k_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.3.2 Print k=10 closests documents to the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_title = \"Tiny telescope's big discovery opens new doors\" # label = 4\n",
    "\n",
    "query = tokenize(query_title).split()\n",
    "query_vector = vectorize_vsr(query, vocabulary, idf)\n",
    "\n",
    "top_k_docs = knn(vectorized_documents, query_vector)\n",
    "\n",
    "for k, doc_index in enumerate(top_k_docs):\n",
    "    print(f\"{k+1} : {original_documents[doc_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.3.3 Implement probabilistic and weigting estimation of kNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_weighting_estimate(doc_vectors, doc_labels, query_vector, k=10):\n",
    "    \"\"\" Weighting estimation for kNN classification\n",
    "    :param doc_vectors: Document vectors (np.array(np.array))\n",
    "    :param doc_labels: Document labels/topics (list)\n",
    "    :param query_vector: Query vector (np.array)\n",
    "    :param k: Number of nearest neighbors to retrieve\n",
    "    \n",
    "    :return: A dictionary containing the estimation score for each label/topic (dict)\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # --------------\n",
    "    return scores\n",
    "\n",
    "def knn_probabilistic_estimate(doc_vectors, doc_labels, query_vector, k=10):\n",
    "    \"\"\" Probabilistic estimation for kNN classification\n",
    "    :param doc_vectors: Document vectors (np.array(np.array))\n",
    "    :param doc_labels: Document labels/topics (list)\n",
    "    :param query_vector: Query vector (np.array)\n",
    "    :param k: Number of nearest neighbors to retrieve\n",
    "    \n",
    "    :return: A dictionary containing the estimation score for each label/topic (dict)\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # --------------\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.3.4 Compute weighting and probabilistic estimation of the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_titles = [(\"Airlines Agree to Cut Flights at Chicago O'Hare\", 3), (\"Stuttgart Closing on Qualification\", 2)]\n",
    "\n",
    "queries = [tokenize(q[0]).split() for q in query_titles] \n",
    "query_vectors = [vectorize_vsr(q, vocabulary, idf) for q in queries]\n",
    "k = 10\n",
    "\n",
    "for i, query_v in enumerate(query_vectors):\n",
    "    print(f\"Query: {query_titles[i]}\")\n",
    "    w_estimate = knn_weighting_estimate(vectorized_documents, documents_topics, query_v, k)\n",
    "    w_estimate = sorted(w_estimate.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"Weighting estimate: {w_estimate}\")\n",
    "   \n",
    "    prob_estimate = knn_probabilistic_estimate(vectorized_documents, documents_topics, query_v, k)\n",
    "    prob_estimate = sorted(prob_estimate.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"Probabilistic estimate: {prob_estimate}\")\n",
    "\n",
    "    print(\"*************************************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.3.5 Compute weighting and probabilistic estimation of the given query for different values of `k`:\n",
    "\n",
    "Discuss the changes in the results by increasing the value of `k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN estimation for different values of k (first query)\n",
    "for k in [1, 5, 10, 15, 20]:\n",
    "    w_estimate = knn_weighting_estimate(vectorized_documents, documents_topics, query_vectors[0], k)\n",
    "    w_estimate = sorted(w_estimate.items(), key=lambda x: x[1], reverse=True)\n",
    "    w_label = w_estimate[0][0]\n",
    "\n",
    "    prob_estimate = knn_probabilistic_estimate(vectorized_documents, documents_topics, query_vectors[0], k)\n",
    "    prob_estimate = sorted(prob_estimate.items(), key=lambda x: x[1], reverse=True)\n",
    "    p_label = prob_estimate[0][0]\n",
    "\n",
    "    print(f\"k: {k}, w_label = {w_label}, p_label: {p_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è PLEASE WRITE YOUR ANSWER HERE**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.3.6 Implement a Rocchio classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocchio_estimate(doc_vectors, doc_labels, query_vector):\n",
    "    \"\"\" \n",
    "    Rocchio classification\n",
    "    :param doc_vectors: Document vectors (np.array(np.array))\n",
    "    :param doc_labels: Document labels/topics (list)\n",
    "    :param query_vector: Query vector (np.array)\n",
    "    \n",
    "    :return: A dictionary containing the estimation score for each label/topic (dict)\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # --------------\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.3.7 Compute Rocchio estimation of the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_estimate = rocchio_estimate(vectorized_documents, documents_topics, query_vectors[0])\n",
    "roc_estimate = sorted(roc_estimate.items(), key=lambda x: x[1])\n",
    "print(f\"Rocchio estimate: {roc_estimate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part34'></a>\n",
    "### PART 3.4: Naive Bayes Classifier\n",
    "\n",
    "*(2 sub-questions)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "def get_topic_tf(tokenized_docs, doc_labels, vocabulary):\n",
    "    \"\"\" It computes term frequency for each topic/label\n",
    "    :param tokenized_docs: List of tokenized documents (list)\n",
    "    :param doc_labels: Document labels/topics (list)\n",
    "    :vocabulary: A dictionary, with keys the words and values the frequency of each word.\n",
    "    :return: A dictionary, with keys the topics/labels and values a dictionary of word frequencies  \n",
    "    \"\"\"\n",
    "    topic_term_freq = {t:{w:0 for w in vocabulary} for t in list(set(doc_labels))}\n",
    "    for i, doc in enumerate(tokenized_docs):\n",
    "        counter = Counter(doc)\n",
    "        for word in vocabulary:\n",
    "            if word in counter:\n",
    "                topic_term_freq[doc_labels[i]][word] += counter[word]\n",
    "    return topic_term_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.4.1 Compute the Naive Bayes estimation for the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navie_bayes_classifier(tf_dict, query, topics_probs):\n",
    "    \"\"\" Naive Bayes classification\n",
    "    :param tf_dict: A dictionary, with keys the topics/labels and values a dictionary of word frequencies  \n",
    "    :param query: Query vector\n",
    "    :param topics_probs: Probaility distribution of each topic/label (dict)\n",
    "    :return: A dictionary containing the log probability estimation for each topic\n",
    "    \"\"\"\n",
    "    # --------------\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # --------------\n",
    "    return log_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probability distribution of topics/labels\n",
    "topics_freq = Counter(documents_topics)\n",
    "topics_probs = {t:topics_freq[t]/len(documents_topics) for t in topics_freq}\n",
    "# Compute word frequency per topic\n",
    "tf_dict = get_topic_tf(tokenized_documents, documents_topics, vocabulary)\n",
    "\n",
    "nb_estimation = navie_bayes_classifier(tf_dict, queries[0], topics_probs)\n",
    "nb_estimation = sorted(nb_estimation.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"Naive Bayes Estimation: {nb_estimation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.4.2 Discuss the difference the above classifers:\n",
    "1. Which kNN classifer is more accurate? Weighting or probabilistic estimation? Why?\n",
    "2. What is the difference between Rocchio and kNN classification?\n",
    "3. When Naive Bayes is prefered over kNN? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**‚û°Ô∏è PLEASE WRITE YOUR ANSWER HERE**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part35'></a>\n",
    "### PART 3.5: Association Rules\n",
    "*(3 sub-questions)*\n",
    "\n",
    "Now we would like to identify frequent rules that govern how words appear together in the news article **titles**.\n",
    "\n",
    "Using the `tokenized_documents` provided before and by considering the pair of words containing _\"microsoft\"_ (we only consider rules of size 2) do the following:\n",
    "\n",
    "* Compute **support** and **confidence** for the rules `microsoft` -> `X`, where X is a word appearing with microsoft in the title of an article.\n",
    "* From the confidence of the rules you obtained, compute **lift**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **3.5.1 Compute support and confidence for the rules `microsoft` -> `X`, where X is a word appearing with microsoft in the title of an article.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# support    = {}\n",
    "# confidence = {}\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **3.5.2 From the confidence of the rules you obtained, compute lift.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# lift = {}\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 3 rules with highest support\n",
    "{k: v for k, v in sorted(support.items(), key=lambda item: item[1], reverse=True)[:3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 3 rules with highest confidence\n",
    "{k: v for k, v in sorted(confidence.items(), key=lambda item: item[1], reverse=True)[:3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 3 rules with highest lift\n",
    "{k: v for k, v in sorted(lift.items(), key=lambda item: item[1], reverse=True)[:3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîö END OF EXAM\n",
    "> Don't forget to change the submitted file with your SciperNo as the file name before submitting.\n",
    "\n",
    "<a id='submit'></a>\n",
    "#### [SUBMIT HERE](https://moodle.epfl.ch/mod/quiz/view.php?id=1235303)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "75fd394e35225182f207b93437350142e41aafd8fa2b11cc1a17258e1fa2f196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
