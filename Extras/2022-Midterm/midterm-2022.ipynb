{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Distributed-Information-Systems\" data-toc-modified-id=\"Distributed-Information-Systems-0\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Distributed Information Systems</a></span></li><li><span><a href=\"#Word-Representation-for-Concept-Identification\" data-toc-modified-id=\"Word-Representation-for-Concept-Identification-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Word Representation for Concept Identification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-the-vocabulary-by-selecting-top-k-frequent-words\" data-toc-modified-id=\"Build-the-vocabulary-by-selecting-top-k-frequent-words-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Build the vocabulary by selecting top-k frequent words</a></span></li><li><span><a href=\"#Construct-the-word-cooccurence-matrix\" data-toc-modified-id=\"Construct-the-word-cooccurence-matrix-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Construct the word cooccurence matrix</a></span></li><li><span><a href=\"#Perform-SVD-on-the-matrix-and-select-the-largest-singular-values\" data-toc-modified-id=\"Perform-SVD-on-the-matrix-and-select-the-largest-singular-values-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Perform SVD on the matrix and select the largest singular values</a></span></li></ul></li><li><span><a href=\"#Vector-based-retrieval-using-Word-representations\" data-toc-modified-id=\"Vector-based-retrieval-using-Word-representations-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vector-based retrieval using Word representations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Document-and-query-vectors-from-word-representations\" data-toc-modified-id=\"Document-and-query-vectors-from-word-representations-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Document and query vectors from word representations</a></span></li><li><span><a href=\"#Retrieve-top-10-relevant-documents\" data-toc-modified-id=\"Retrieve-top-10-relevant-documents-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Retrieve top-10 relevant documents</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluate-retrieval-result-using-DCG\" data-toc-modified-id=\"Evaluate-retrieval-result-using-DCG-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Evaluate retrieval result using DCG</a></span></li><li><span><a href=\"#Explain-the-DCG-values-plot\" data-toc-modified-id=\"Explain-the-DCG-values-plot-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Explain the DCG values plot</a></span></li></ul></li><li><span><a href=\"#Submit-your-notebook\" data-toc-modified-id=\"Submit-your-notebook-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Submit your notebook</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Information Systems\n",
    "***Midterm Exam, Fall-Winter Semester 2021-22***\n",
    "\n",
    "The following materials are allowed: exercise sheets and solutions, past exams with your own solution, personally written notes and personally collected documentation.\n",
    "\n",
    "The exam will be held on your computer, but digital communication by any means is strictly prohibited. \n",
    "By participating to this exam you agree to these conditions.\n",
    "\n",
    "These are the instructions for the exam:\n",
    "\n",
    "1. You are not allowed to leave the examination room in the first 20 and the last 15 minutes of the exam.\n",
    "* We will publish 15 minutes before the end of the exam a password for uploading your solutions on Moodle.\n",
    "* It is not recommended to leave the exam before the password is published. If you need to leave earlier, contact us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required libraries\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')).union(set(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    '''Reads corpus from files.'''\n",
    "    \n",
    "    documents = []\n",
    "    orig_docs = []\n",
    "    DIR = './'\n",
    "    tknzr = TweetTokenizer()\n",
    "    with open(\"epfldocs.txt\", encoding = \"utf-8\") as f:\n",
    "        content = f.readlines()\n",
    "    for text in content:\n",
    "        orig_docs.append(text)\n",
    "        # split into words\n",
    "        tokens = tknzr.tokenize(text)\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "\n",
    "        documents.append(' '.join(words))\n",
    "    return documents, orig_docs\n",
    "\n",
    "documents, orig_docs = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(documents) == 1075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation for Concept Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build word representations in a latent concept space using SVD. Differently to Latent Semantic Indexing (LSI) we will derive the latent concepts space from the **word co-occurrence matrix** (and not from the term-document matrix, as in standard LSI).\n",
    "\n",
    "An entry (i,j) in the word co-occurrence matrix corresponds to the number of times the word i co-occurs with the word j in the context of word i. The context of the words consist of the words preceding or succeeding the word in the text.  \n",
    "\n",
    "By deriving an SVD from the word co-occurrence matrix, and selecting the top dimensions of the latent space, we obtain a word representation as vectors over a concept space. Commonly such word representations are also called word embeddings.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary by selecting top-k frequent words\n",
    "No code is required for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary is the list of all words\n",
    "# vocabulary_to_index maps words to their index\n",
    "\n",
    "def create_vocabulary_frequency(corpus, vocab_len):\n",
    "    '''Select top-k (k = vocab_len) words in term of frequencies as vocabulary'''\n",
    "    vocabulary_to_index = {}\n",
    "    count = defaultdict(int)\n",
    "    for document in corpus:\n",
    "        for word in document.split():\n",
    "                count[word] += 1\n",
    "    \n",
    "    sorted_count_by_freq = sorted(count.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    vocabulary = []\n",
    "    for i, x in enumerate(sorted_count_by_freq[:vocab_len]):\n",
    "        vocabulary.append(x[0])\n",
    "        vocabulary_to_index[x[0]] = i\n",
    "    return vocabulary, vocabulary_to_index\n",
    "\n",
    "vocab_freq, vocabulary_to_index = create_vocabulary_frequency(documents, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the word cooccurence matrix\n",
    "\n",
    "In this question, you need to construct the word co-occurence matrix, given the vocabulary and the set of documents.\n",
    "\n",
    "The value of a cell (i,j) is the number of times the word i co-occurs with the word j in the context of word i.\n",
    "\n",
    "For this question, a word $w_i$ cooccurs with a word $w_j$ in the context of word $w_i$ if $w_j$ preceeds or succeeds $w_i$ with a distance **at most 2**.\n",
    "\n",
    "Example: For this document \"*how to bake bread without bake recip*\", the words coocur with the word \"*bread*\" are \"*to, bake, without, bake*\".\n",
    "\n",
    "Make sure that you consider only words that appear in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_word_cooccurence_matrix(vocabulary_to_index, documents, k=2):\n",
    "    matrix = np.zeros((len(vocabulary_to_index), len(vocabulary_to_index)))\n",
    "    for document in documents:\n",
    "        terms = document.split()\n",
    "        for ind, term_i in enumerate(terms):\n",
    "            pass\n",
    "            # YOUR CODE HERE\n",
    "    return matrix\n",
    "\n",
    "word_cooccur_matrix = construct_word_cooccurence_matrix(vocabulary_to_index, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally check whether the matrix you constructed is correct using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_matrix = False\n",
    "if assert_matrix:\n",
    "    word_coor_mat = np.load(\"word_coocur_matrix.npy\")\n",
    "    assert(word_coor_mat == word_cooccur_matrix[:100,:100]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform SVD on the matrix and select the largest singular values \n",
    "\n",
    "We perform SVD on the matrix $\\mathbf{M} = \\mathbf{K}\\mathbf{S}\\mathbf{D}^T$ and select the first 128 largest singular values.\n",
    "\n",
    "Then, we can use the submatrix $\\mathbf{K_s}$, corresponding to the largest singular values, as the word representation matrix. \n",
    "\n",
    "Hint 1 : Are the words represented in $\\mathbf{K_s}$ as rows or columns?\n",
    "\n",
    "Hint 2: np.linalg.svd(M, full_matrices=False) performs SVD on the matrix $\\mathbf{M}$ and returns $\\mathbf{K}, \\mathbf{S}, \\mathbf{D}^T$\n",
    "\n",
    " -  $\\mathbf{K}, \\mathbf{D}^T$ are matrices with orthonormal columns\n",
    " -  $\\mathbf{S}$ is a **vector** of singular values in a **descending** order\n",
    " \n",
    "Hint 3: np.diag(V) converts a vector to a diagonal matrix\n",
    "\n",
    "Hint 4: To select:\n",
    " - the first k rows of a matrix A, use A[0:k, :]\n",
    " - the first k columns of a matrix A, use A[:, 0:k]\n",
    " - the submatrix from first k rows and k columns of a matrix A, use A[0:k, 0:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a word coocurrence matrix and the number of singular values that will be selected\n",
    "# Output: K_s, S_s, Dt_s are similar to the defintion in the lecture\n",
    "\n",
    "def truncated_svd(word_cooccur_matrix, num_val):\n",
    "    # The following may take 1-2 minutes since we are decomposing a matrix of size 5000x1075\n",
    "    K, S, Dt = np.linalg.svd(word_cooccur_matrix, full_matrices=False) \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    K_sel = ...\n",
    "    S_sel = ...\n",
    "    Dt_sel = ...\n",
    "    return K_sel, S_sel, Dt_sel\n",
    "\n",
    "K_s, S_s, Dt_s = truncated_svd(word_cooccur_matrix,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector-based retrieval using Word representations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document and query vectors from word representations\n",
    "\n",
    "For each document and query, we construct the corresponding vector by **averaging** its word representations.\n",
    "\n",
    "Hint: not all words are in the vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vecs(documents, word_embedding_matrix, vocabulary_to_index):\n",
    "    doc_vecs = np.zeros((len(documents), word_embedding_matrix.shape[1]))\n",
    "    # YOUR CODE HERE\n",
    "    return doc_vecs\n",
    "\n",
    "# YOUR CODE HERE\n",
    "doc_vecs = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve top-10 relevant documents\n",
    "\n",
    "Retrieve top-10 relevant documents for the query \"*computer science*\"\n",
    "\n",
    "Hint: you may use the function get_doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"computer science\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "query_vec = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy*1.0/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "def retrieve_documents(doc_vecs, query_vec, top_k):\n",
    "    scores = [[cosine_similarity(query_vec, doc_vecs[d,:]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    doc_ids = []\n",
    "    retrieved = []\n",
    "    for i in range(top_k):\n",
    "        doc_ids.append(scores[i][1])\n",
    "        retrieved.append(orig_docs[scores[i][1]])\n",
    "    return doc_ids, retrieved\n",
    "\n",
    "retrieved_ids, retrieved_docs = retrieve_documents(doc_vecs, query_vec, top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "We consider the scikit reference code as an “oracle” that supposedly gives the correct result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval oracle \n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), vocabulary=vocab_freq, min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(documents)\n",
    "npm_tfidf = features.todense()\n",
    "\n",
    "# Return all document ids that that have cosine similarity with the query larger than a threshold\n",
    "def search_vec_sklearn(query, features, threshold=0.1):\n",
    "    new_features = tf.transform([query])\n",
    "    cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    return doc_ids\n",
    "\n",
    "# gt_ids are the document ids retrieved by the oracle\n",
    "gt_ids = search_vec_sklearn(query, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also assume that there is a user that has done the grading of all the documents according to their relevance. \n",
    "The top-10 results using scikit-learn have grade 3, the next 10 results have grade 2, \n",
    "the rest in the list has grade 1 while non-relevant results have grade 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade = []\n",
    "for i in range(len(documents)):\n",
    "    if i in gt_ids[:10]:\n",
    "        grade.append(3)\n",
    "    elif i in gt_ids[10:20]:\n",
    "        grade.append(2)\n",
    "    elif i in gt_ids[20:]:\n",
    "        grade.append(1)\n",
    "    else:\n",
    "        grade.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate retrieval result using DCG \n",
    "\n",
    "Discounted Cumulative Gain (DCG) is a retrieval metric that also takes into account the ordering of the result. \n",
    "\n",
    "The DCG accumulated at a rank $k$ is defined as:\n",
    "\n",
    "$DCG_k = \\sum_{i=1}^k \\frac{grade[i]}{log_2(i+1)}$\n",
    "\n",
    "where $grade[i]$ is the relevance score given by the user for the result at position $i$.\n",
    "\n",
    "Hint: the logarithm is computed using the function np.log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(k, retrieved_ids, grade):\n",
    "    dcg_val = ...\n",
    "    # YOUR CODE HERE\n",
    "    return dcg_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the DCG for the top-1 to the top-10 retrieval results and we plot the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "for i in range(10):\n",
    "    val = dcg(i, retrieved_ids, grade)\n",
    "    vals.append(val)\n",
    "    \n",
    "plt.plot(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the DCG values plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit your notebook\n",
    "\n",
    "Go to [Moodle](https://moodle.epfl.ch/course/view.php?id=4051) > Exams > Midterm and follow the instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "400px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
