{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Information Systems\n",
    "### Prof. Karl Aberer <br> Final Exam, Spring Semester 2018\n",
    "\n",
    "The following materials are allowed: exercise sheets\n",
    "and solutions, past exams with your own solution, personally written notes and personally collected documentation.\n",
    "  \n",
    "The exam will be held on your computer, but digital communication by any means is strictly prohibited. You are not allowed to use any form of Web browsing, except to connect to Moodle and to download and upload the exam. By participating to this exam you agree to these conditions.\n",
    "\n",
    "These are the instructions for the exam:\n",
    "1. You are not allowed to leave the examination room in the first 20 and the last 15 minutes of the exam.\n",
    "* We will publish 15 minutes before the end of the exam a password for uploading your solutions on Moodle.\n",
    "* It is not recommended to leave the exam before the password is published. If you need to leave earlier contact us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Multiple Choice Questions\n",
    "\n",
    "Answer the [multiple choice questions in Moodle](https://moodle.epfl.ch/mod/quiz/view.php?id=988129) as in the quizzes.\n",
    "\n",
    "The password for answering the quiz is: **wBHNcJZd**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: User Classification in Social Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we would like to identify Twitter user accounts $T$ that have interest in Swiss universities.\n",
    "\n",
    "Given that we have no training data, we analyse the social relationships of those users.\n",
    "\n",
    "More specifically, we consider that users that follow an Twitter account $S$ related to a Swiss University (e.g. @EPFL, @EPFL\\_en, @ETHZ etc.) show interest for Swiss universities. We assume that $S$ and $T$ are disjoint. We capture the follower relationship in a matrix $F$, where an entry $f_{ij} = 1$ indicates a follower link from user account $i\\in T$ to university account $j\\in S$. Otherwise the entry is zero.\n",
    "\n",
    "We would like to assess the level of interest of users into Swiss universities by using two methods:\n",
    "1. The HITS algorithm, where we consider the Swiss university accounts as authorities and the users we intend to classify as hubs. A high hub value $t_i$ for user $i\\in T$ should indicate a high level of interest in Swiss universities, a high authority value $s_j$ indicates that university account $j\\in S$ is a good indicator for interest in Swiss universities.\n",
    "2. The EM algorithm, where we consider the existence of a follower link from a user to a Swiss university account as a positive feedback given that university account on the interest of the user , and the absence of such a link as negative feedback (i.e. we consider the university accounts like experts and the users like the item to be evaluated). A high level of positive feedback $t_i$ for user $i\\in T$ indicates interest in Swiss universities, and a high expertise value $s_j$ indicates that university account $j\\in S$ is a good indicator for interest in Swiss universities.\n",
    "\n",
    "### Questions\n",
    "\n",
    "1) Formulate the two algorithms (HITS and EM) using the following patterns for a recursive computation of $t$ and $s$\n",
    "\n",
    "HITS:  $\\vec{t}_{k+1} = r_1  M_1. \\vec{v_1},\\ \\vec{s}_{k+1} = r_2  M_2 . \\vec{v_2}$\n",
    "\n",
    "EM:    $\\vec{t}_{k+1} = r_3  M_3 . \\vec{v_3},\\ \\vec{s}_{k+1} = r_4 (M_4.\\vec{v_4} + M_5.\\vec{v_5})$\n",
    "\n",
    "$\\vec{t}=(t_1,\\ldots,t_n)$ and $\\vec{s}=(s_1,\\ldots,s_m)$ are the vectors ranking interest and expertise of users and universities. Your task is to express $r_p$, $\\vec{v_p}$ and $M_p$, $p=1,\\ldots,5$ in terms of $\\vec{t}_{k}$, $\\vec{s}_{k}$ and $F$, where $r_p$ are scalars, $\\vec{v_p}$ are vectors and $M_p$ are matrices.\n",
    "\n",
    "You can use also the following functions:\n",
    "\n",
    "$|\\vec{v}|_2$ for the 2-norm\n",
    "\n",
    "$|\\vec{v}|_1$ for the 1-norm\n",
    "\n",
    "$M^t$ for the matrix transpose\n",
    "\n",
    "$round(\\vec{v})$ for rounding the values of a vector in $[0,1]$ to $0$ and $1$\n",
    " \n",
    "2) For HITS: in which sense is the input to the algorithm a special case of the most general possible inputs? Which additional constraints does the formulation of the problem satisfy?\n",
    "\n",
    "3) Assume after applying HITS or EM you perform a binary classification of the accounts by setting a threshold on the values in $\\vec{t}$, i.e., all accounts with a authority/interest value above the threshold are considered as accounts interested in Swiss universities. Identify and shortly describe a simple, standard classification algorithm that uses the accounts with binary labels as training data to classify new accounts and that does not require training (and does not rely on recomputing HITS or EM).\n",
    "  \n",
    "Please provide your answers in the cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:  Betweenness Centrality in Social Networks\n",
    "  \n",
    "  \n",
    "\"Les misÃ©rables\" is a novel written by Victor Hugo and is considered as one of the greatest novels of the 19th century. In this question, you will analyze the social network composed by the protagonists of the novel. The character network represents the interactions among character and has been created, by hand, by Donald E. Knuth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import operator\n",
    "import networkx as nx\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from dis_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = ex1_load_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex1_draw(G, nx.betweenness_centrality(G))\n",
    "centr = nx.betweenness_centrality(G,normalized=False)\n",
    "\n",
    "ex1_print_topn(centr,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "In this problem, you will have to implement node betweeness centrality, a measure that provide a score for every node $n$ of a graph $G$.  \n",
    "  \n",
    "Betweenness centrality $c_B(n)$ of a node $n$ is the sum of the fraction of all-pairs shortest paths that pass through $n$:  \n",
    "  \n",
    "$\\Large c_B(n) =\\sum_{s,t \\in N} \\frac{\\sigma(s, t|n)}{\\sigma(s, t)}$\n",
    "  \n",
    "where $N$ is the set of nodes, $\\sigma(s, t)$ is the number of shortest paths connecting $s$ and $t$, and $\\sigma(s, t|n)$ is the number of paths passing through node $n$, provided $n$ is different from $s$ and $t$. For the other cases $\\sigma(s, t|n)$ is defined as follows: \n",
    "* If $s = t$: $\\sigma(s, t) = 1$\n",
    "* If $n \\in \\{s, t\\}$: $\\sigma(s, t|n) = 0$\n",
    "\n",
    "Note that this definition of node betweenness employs no normalization.\n",
    "  \n",
    "### Code Cheat Sheet\n",
    "\n",
    "Here we provide a few explainations about some of the functions you might need.\n",
    "\n",
    "The function ``enumerate(['a','b','c'])`` provides a convenient way to create an index while iterating over a list. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,n in enumerate(['Napoleon','Myriel','Mlle.Baptistine','Mme.Magloire']):\n",
    "    print(i,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ``nx.all_shortest_paths(G,s,t)`` returns a list of all the shortest paths between nodes $s$ and $t$, given the graph $G$. Note that you can directly use node labels to specify the nodes, such as in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in nx.all_shortest_paths(G,'Tholomyes','Mme.Magloire'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1) Fill the code skeleton below to compute the betweenness centrality for every node of the graph.   \n",
    "\n",
    "2) The proposed code skeleton has an unnecessary high time complexity. Can you propose a solution to decrease it? Please answer in the cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Does the PageRank algorithm always produce the same values as node betweenness, after normalizing the node betweenness values to the interval [0,1]? For PageRank, consider that every edge of the undirected graph is replaced by two directed edges going in both directions. Provide an answer and an argument to support your answer in the cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "characters = list(G.nodes())\n",
    "result   = {}\n",
    "\n",
    "# compute a score fo every character\n",
    "for n in characters:\n",
    "\n",
    "    # accumulator for the score of the current character\n",
    "    accum = 0\n",
    "\n",
    "    # shortest path FROM every character...\n",
    "    for i,s in enumerate(characters):\n",
    "        # ..TO every character\n",
    "        for j,t in enumerate(characters):\n",
    "            # we do not consider the same path twice\n",
    "            if j > i:\n",
    "                continue\n",
    "            \n",
    "            try:  \n",
    "                all_short = nx.all_shortest_paths(G,s,t)\n",
    "                \n",
    "                ### SOLUTION ####################################\n",
    "                num   = 0\n",
    "                denum = 0\n",
    "                for p in all_short:\n",
    "\n",
    "                    px = set(p)\n",
    "                    px = px - set([s])\n",
    "                    px = px - set([t])\n",
    "                    if n in px:\n",
    "                        num += 1\n",
    "                    denum += 1\n",
    "                    \n",
    "                if s==t:\n",
    "                    denum = 1\n",
    "\n",
    "                if n==s or n==t:\n",
    "                    num = 0\n",
    "                \n",
    "                accum += float(num)/float(denum)\n",
    "                ### END SOLUTION ################################\n",
    "                \n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "                \n",
    "    result[n] = accum\n",
    "    \n",
    "ex1_print_topn(result,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Sentiment Classification of Movie Reviews\n",
    "The goal of this problem is to build an end-to-end _Sentiment Classification_ pipeline. In order to achieve this, you will have to go through the subtasks that are indicated below and fill in the code in the respective placeholders. Notice that you are not allowed to use external libraries other than the ones that are already imported in the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Required libraries.\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    '''Reads corpus from files.'''\n",
    "    \n",
    "    neg_reviews = []\n",
    "    pos_reviews = []\n",
    "    DIR = 'movie_reviews/'\n",
    "\n",
    "    for l, sent in [(neg_reviews, 'neg'), (pos_reviews, 'pos')]:\n",
    "        for file in os.listdir(DIR+sent):\n",
    "            with open(os.path.join(DIR+sent, file)) as f:\n",
    "                text = f.read()\n",
    "            # split into words\n",
    "            tokens = word_tokenize(text)\n",
    "            # convert to lower case\n",
    "            tokens = [w.lower() for w in tokens]\n",
    "            # remove punctuation from each word\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            stripped = [w.translate(table) for w in tokens]\n",
    "            # remove remaining tokens that are not alphabetic\n",
    "            words = [word for word in stripped if word.isalpha()]\n",
    "            # filter out stop words\n",
    "            words = [w for w in words if not w in stop_words]\n",
    "\n",
    "            l.append(' '.join(words))\n",
    "    return neg_reviews, pos_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Split your corpus into a training and a test set with ratio 9:1. \n",
    "* _Hint: Make sure that both sets are equally populated w.r.t. the two sentiments._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_reviews, pos_reviews = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### ADD YOUR CODE HERE ###############\n",
    "neg_reviews_train = []\n",
    "pos_reviews_train = []\n",
    "\n",
    "neg_reviews_test = []\n",
    "pos_reviews_test = []\n",
    "\n",
    "reviews_train = []\n",
    "reviews_test = []\n",
    "############### ADD YOUR CODE HERE ###############\n",
    "\n",
    "# ############### SOLUTION ###############\n",
    "# neg_reviews_train = neg_reviews[:int(len(neg_reviews)*0.9)]\n",
    "# pos_reviews_train = pos_reviews[:int(len(pos_reviews)*0.9)]\n",
    "\n",
    "# neg_reviews_test = neg_reviews[int(len(neg_reviews)*0.9):]\n",
    "# pos_reviews_test = pos_reviews[int(len(pos_reviews)*0.9):]\n",
    "\n",
    "# reviews_train = neg_reviews_train + pos_reviews_train\n",
    "# reviews_test = neg_reviews_test + pos_reviews_test\n",
    "# ############### SOLUTION ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Transform the reviews of the training set into vectors using the Bag-Of-Words model. Each dimension of a vector must count the number of occurences of a word in the review.\n",
    "* _Hint 1: Make sure that all the vectors belong to the same space._\n",
    "* _Hint 2: For this task you can reuse code from the first exercise of DIS2018 (Vector Space Retrieval)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(corpus, min_tf):\n",
    "    '''Creates the vocabulary (set of distinct words) that appear more than $min_tf$ times in the corpus.'''\n",
    "    \n",
    "    count = {}\n",
    "    for document in corpus:\n",
    "        for word in document.split():\n",
    "            if word in count:\n",
    "                count[word] += 1\n",
    "            else:\n",
    "                count[word] = 0\n",
    "    \n",
    "    vocabulary = []\n",
    "    for w, c in count.items():\n",
    "        if c > min_tf:\n",
    "            vocabulary.append(w)\n",
    "    \n",
    "    return vocabulary\n",
    "    \n",
    "vocabulary = create_vocabulary(reviews_train, len(reviews_train)*0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_vector(reviews):\n",
    "    '''Transforms a list of reviews to a list of vectors.'''\n",
    "    \n",
    "    reviews_vec = []\n",
    "\n",
    "    for review in reviews:\n",
    "        \n",
    "        \n",
    "        ############### ADD YOUR CODE HERE ###############\n",
    "        vector = []\n",
    "        ############### ADD YOUR CODE HERE ###############\n",
    "        \n",
    "#         ############### SOLUTION ###############\n",
    "#         count = {}\n",
    "#         for word in review.split():\n",
    "#             if word in count:\n",
    "#                 count[word] += 1\n",
    "#             else:\n",
    "#                 count[word] = 0\n",
    "\n",
    "#         #move to vocabulary space\n",
    "#         vector = [0]*len(vocabulary)\n",
    "#         for i,term in enumerate(vocabulary):\n",
    "#             if term in count:\n",
    "#                 vector[i] = count[term]\n",
    "        \n",
    "#         ############### SOLUTION ###############\n",
    "\n",
    "        reviews_vec.append(vector)\n",
    "\n",
    "    return reviews_vec\n",
    "\n",
    "neg_reviews_train_vec = transform_to_vector(neg_reviews_train)\n",
    "pos_reviews_train_vec = transform_to_vector(pos_reviews_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 \n",
    "Aggregate the vectors of the positive and negative reviews of the training set into a single vector. This vector will be the representative vector of each of the two sentiments. Which aggregation function did you choose and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_vector_list(vlist, aggfunc):\n",
    "    if aggfunc == 'max':\n",
    "        return np.array(neg_reviews_train_vec).max(axis=0)\n",
    "    elif aggfunc == 'min':\n",
    "        return np.array(neg_reviews_train_vec).min(axis=0)\n",
    "    elif aggfunc == 'mean':\n",
    "        return np.array(neg_reviews_train_vec).mean(axis=0)\n",
    "    else:\n",
    "        return np.zeros(np.array(neg_reviews_train_vec).shape[1])\n",
    "\n",
    "############### ADD YOUR CODE HERE ###############\n",
    "neg_reviews_repr = []\n",
    "pos_reviews_repr = []\n",
    "############### ADD YOUR CODE HERE ###############\n",
    "\n",
    "# ############### SOLUTION ###############\n",
    "# neg_reviews_repr = aggregate_vector_list(neg_reviews_train_vec, 'mean')\n",
    "# pos_reviews_repr = aggregate_vector_list(pos_reviews_train_vec, 'mean')\n",
    "# ############### SOLUTION ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "Transform the reviews of the movies in the test set into vectors. \n",
    "* _Hint: Make sure that all the vectors belong to the same space as the ones from the training set._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### ADD YOUR CODE HERE ###############\n",
    "neg_reviews_test_vec = []\n",
    "pos_reviews_test_vec = []\n",
    "############### ADD YOUR CODE HERE ###############\n",
    "\n",
    "# ############### SOLUTION ###############\n",
    "# neg_reviews_test_vec = transform_to_vector(neg_reviews_test)\n",
    "# pos_reviews_test_vec = transform_to_vector(pos_reviews_test)\n",
    "# ############### SOLUTION ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "Consider the following Naive Bayes sentiment classifier:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(sentiment | review)   = \\frac{P(review | sentiment) * P(sentiment)}{P(review)}\n",
    "\\end{equation*}\n",
    "\n",
    "Assume that you can estimate the likelihood probability as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(review | sentiment) = cosine\\_similarity(review\\_vec, sentiment\\_representative\\_vec)\n",
    "\\end{equation*}\n",
    "\n",
    "1) How do you take into account the probabilities: $P(sentiment)$ and $P(review)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implement the classifier and compute its performance in terms of false negatives, true negatives, falsepositives and true positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "tp = 0\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    '''Computes the cosine similarity of $a$ and $b$.'''\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "    \n",
    "############### ADD YOUR CODE HERE ###############\n",
    "\n",
    "############### ADD YOUR CODE HERE ###############\n",
    "\n",
    "# ############### SOLUTION ###############\n",
    "# def naive_bayes(review_vec):\n",
    "#     if  cosine_similarity(np.array(review_vec), neg_reviews_repr) > cosine_similarity(np.array(review_vec), pos_reviews_repr):\n",
    "#         return 'neg'\n",
    "#     else:\n",
    "#         return 'pos'\n",
    "\n",
    "# for review_vec in neg_reviews_test_vec:\n",
    "#     if naive_bayes(review_vec) == 'neg':\n",
    "#         tn +=1\n",
    "#     else:\n",
    "#         fp +=1\n",
    "\n",
    "# for review_vec in pos_reviews_test_vec:\n",
    "#     if naive_bayes(review_vec) == 'neg':\n",
    "#         fn +=1\n",
    "#     else:\n",
    "#         tp +=1\n",
    "# ############### SOLUTION ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "Compute the _F-measure_ to evaluate your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### ADD YOUR CODE HERE ###############\n",
    "precision = 0\n",
    "recall = 0\n",
    "f_measure = 0\n",
    "print('F-measure: ', f_measure)\n",
    "############### ADD YOUR CODE HERE ###############\n",
    "\n",
    "# ############### SOLUTION ###############\n",
    "# precision = tp/(tp+fp)\n",
    "# recall = tp/(tp+fn)\n",
    "# f_measure = 2*precision*recall/(precision+recall)\n",
    "# print('F-measure: ', f_measure)\n",
    "# ############### SOLUTION ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
